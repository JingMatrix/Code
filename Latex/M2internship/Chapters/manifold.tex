%! TEX root = ../barycenter.tex
\chapter{Wasserstein space over Riemannian manifold}
\section{Convex analysis on Riemannian manifold}
\subsection{Polar factorization of maps on manifolds}

\begin{lem} [Lipschitz cost]
	\label{lem:Lipschitz_cost}
	Let \( ( M , d ) \) be a metric space whose diameter \( | M | : = \sup \{ d ( x , z ) \mid x , z \in M \} \) is finite.
	For each \( y \in M \), the function \( \psi ( x ) = d ^ { 2 } ( x , y ) / 2 \) is Lipschitz continuous:
	\begin{equation}
		\label{equa:Lipschitz}
		| \psi ( x ) - \psi ( z ) | \leq | M | d ( x , z ).
	\end{equation}
\end{lem}

\begin{proof}
	The triangle inequality shows that \( \phi ( x ) : = d ( x , y ) \) has Lipschitz constant one:
	\[ \phi ( x ) - \phi ( z ) = d ( x , y ) - d ( z , y ) \leq d ( x , z ) \]
	for all \( x , z \in M\).
	Also, \( \phi ( x ) = d ( x , y ) \leq | M | < \infty \) is bounded.
	The desired estimate \cref{equa:Lipschitz} then follows easily for \( \psi ( x ) = \phi ^ { 2 } ( x ) / 2 \) :
	\begin{align*}
		2 | \psi ( x ) - \psi ( z ) | & = | \phi ( x ) ( \phi ( x ) - \phi ( z ) ) + \phi ( z ) ( \phi ( x ) - \phi ( z ) ) | \\
		                              & \leq | M | d ( x , z ) + | M | d ( x , z )
	\end{align*}
\end{proof}

\begin{lem} [Infimal convolutions are Lipschitz]
	\label{lem:infimal_convolution_Lipschitz}
	Fix a metric space \( ( M , d ) \) having finite diameter.
	Any \( \psi : M \rightarrow \mathbf { R } \cup \{ \pm \infty \} \) given by an infimal convolution \( \psi = \psi ^ { c c } \) with \( c ( x , y ) = d ^ { 2 } ( x , y ) / 2 \) is either identically infinite \( \psi = \pm \infty \) or Lipschitz continuous throughout \( M\).
	Indeed, it satisfies \cref{equa:Lipschitz}.
\end{lem}

\begin{proof}
	More generally, suppose \( \psi = \phi ^ { c } \) for some \( \phi : M \rightarrow \mathbf { R } \cup \{ \pm \infty \} \) meaning
	\begin{equation}
		\label{equa:infimal_convolution}
		\psi ( x ) = \inf _ { y \in M } c ( x , y ) - \phi ( y )
	\end{equation}
	Observe \( 0 \leq c ( x , y ) \leq | M | ^ { 2 } / 2 \) is bounded.
	Either \( \phi \) is unbounded above, in which case \( ( 11 ) \) yields \( \psi = - \infty \) and the lemma holds trivially, or else \( \psi \) is bounded below.
	Fix \( z \in M \), and note \( \psi ( z ) = + \infty \) in \cref{equa:infimal_convolution} occurs only if \( \phi : = - \infty \) everywhere, in which case \( \psi = + \infty \) again holds trivially.
	Thus we may assume that \( \psi \) is finite everywhere.
	Given any \( \epsilon > 0 \), there exists \( y \in M \)
	such that \( \psi ( z ) + \epsilon \geq c ( z , y ) - \phi ( y ) \), while \( \psi ( x ) \leq c ( x , y ) - \phi ( y ) \) holds because of \cref{equa:infimal_convolution}.
	Subtracting these two inequalities yields
	\begin{align*}
		\psi ( x ) - \psi ( z ) & \leq c ( x , y ) - c ( z , y ) + \epsilon \\
		                        & \leq | M | d ( x , z ) + \epsilon
	\end{align*}
	by \cref{lem:Lipschitz_cost}. Since the last inequality holds for all \( \epsilon > 0 \), the Lipschitz
	estimate \cref{equa:Lipschitz} has been proved.
\end{proof}

One reason to study square distance on manifold could be that under Taylor expansion, it is directlt related to inner product of Riemannian metric.
An example is to prove superdifferentiability of geodesic distance squared in \cite{mccann2001polar}.

\begin{prop}
	[Superdifferentiability of geodesic distance squared]
	Let \( ( M , g ) \) be a \( C ^ { 3 } \)-smooth Riemannian manifold, possibly with boundary.
	Suppose \( \sigma : [ 0,1 ] \rightarrow M \) has minimal length among piecewise \( C ^ { 1 } \) curves joining \( y = \sigma ( 0 ) \) to \( x = \sigma ( 1 ) \notin \partial M \), parameterized with constant speed.
	Then \( \psi ( \cdot ) = d ^ { 2 } ( \cdot , y ) / 2 \) has supergradient \( \dot { \sigma } ( 1 ) \in \bar { \partial } \psi _ { x } \) at \( x \).
\end{prop}

\begin{proof}
	Since \( x \) lies in the interior of \( M \), there is some \( \epsilon > 0 \) and neighbourhood \( X \subset M \) of \( x \) such that: at each \( z \in X \), the exponential map exp \( _ { z } \) maps
	the ball \( \mathbf { B } ( \mathbf { 0 } , \epsilon ) \subset T M _ { z } \) diffeomorphically onto some open set \( U _ { z } \supset X \), as in Milnor.
	The proposition will first be established when
	\( y = \sigma ( 0 ) \in X \), in which case \( \psi \) is actually differentiable at \( \exp _ { y } \dot { \sigma } ( 0 ) = x \)
	We compute its derivative by linearizing exp \( _ { x } \mathbf { v } \in X \) around the origin and $\exp _ { y }$ around $ \dot { \sigma } ( 0 )$:
	\begin{align*}
		\psi \left( \exp _ { x } \mathbf { v } \right) & = d ^ { 2 } \left( y , \exp _ { y } \left( \exp _ { y } ^ { - 1 } \exp _ { x } \mathbf { v } \right) \right) / 2 \\ & = \left| \exp _ { y } ^ { - 1 } \left( \exp _ { x } \mathbf { v } \right) \right| _ { y } ^ { 2 } / 2 \\ & = \left| \dot { \sigma } ( 0 ) + D \left( \exp _ { y } ^ { - 1 } \right) _ { x } D \left( \exp _ { x } \right) _ { 0 } \mathbf { v } + o \left( | \mathbf { v } | _ { x } \right) \right| _ { y } ^ { 2 } / 2 \\ & = | \dot { \sigma } ( 0 ) | _ { y } ^ { 2 } / 2 + g \left\langle \dot { \sigma } ( 0 ) , \left( D \exp _ { y } \right) _ { \dot { \sigma } ( 0 ) } ^ { - 1 } I \mathbf { v } \right\rangle _ { y } + o \left( | \mathbf { v } | _ { x } \right) \\ & = d ^ { 2 } ( x , y ) / 2 + g \langle \dot { \sigma } ( 1 ) , \mathbf { v } \rangle _ { x } + o \left( | \mathbf { v } | _ { x } \right)
	\end{align*}
	so that \( \nabla \psi ( x ) = \dot { \sigma } ( 1 )\).

	Here the last equation follows from \( \dot { \sigma } ( 1 ) = \)
	\( D \left( \exp _ { y } \right) _ { \dot { \sigma } ( 0 ) } \dot { \sigma } ( 0 ) \) and Gauss' lemma.
\end{proof}

\subsection{A Riemannian interpolation inequality Ã  la Borell, Brascamp and Lieb}

We copy following results (tex code) from \cite{cordero2001riemannian}.

\begin{prop}
	[Distances fail to be semiconvex at the cut locus]
	\label{prop:distance_cut_locus}
	At each \( x \in \operatorname { cut } ( y ) \), the square distance \( \psi : = d _ { y } ^ { 2 } / 2 \) satisfies:
	\[ \inf _ { 0 < | v | < 1 } \frac { \psi \left( \exp _ { x } v \right) + \psi \left( \exp _ { x } - v \right) - 2 \psi ( x ) } { | v | ^ { 2 } } = - \infty \]
\end{prop}
\begin{defn} [$c$-transforms and the subset \( \mathcal { I } ^ { c } ( X , Y ) \) of \( c \)-concave functions]
	Let \( X \) and \( Y \) be two compact subsets of \( M \). The set \( \mathcal{I} ^ { c } ( X , Y ) \) of \( c \) -concave functions (relative to \( X \) and \( Y \) ) is the set of functions \( \phi \) : \( X \rightarrow \mathbf { R } \cup \{ - \infty \} \) not identically \( - \infty \), for which there exists a function \( \psi : Y \rightarrow \mathbf { R } \cup \{ - \infty \} \) such that
	\begin{equation}
		\label{defn:c_transform}
		\phi ( x ) = \inf _ { y \in Y } c ( x , y ) - \psi ( y ) \quad \forall x \in X.
	\end{equation}
	We refer to \( \phi \) as the \( c \)-transform of \( \psi \) and abbreviate \cref{defn:c_transform} by writing \( \phi = \psi ^ { c } \)
	Similarly, given \( \phi \in I ^ { c } ( X , Y ) \), we define its \( c \) -transform \( \phi ^ { c } \in \mathcal{I} ^ { c } ( Y , X ) \) by
	\[ \phi ^ { c } ( y ) : = \inf _ { x \in X } c ( x , y ) - \phi ( x ) \quad \forall y \in Y. \]
\end{defn}

\begin{thm}[Optimal mass transport on manifolds]
	Let \( M \) be a complete, continuously curved Riemannian manifold. Fix two Borel probability measures \( \mu \ll \) vol and \( v \) on \( M \), and two compact subsets \( X \) and \( Y \subset M \) containing the support of \( \mu \) and \( v \), respectively. Then there exists \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \) such that the map
	\begin{equation}
		\label{equa:transform_map}
		F ( x ) : = \exp _ { x } ( - \nabla \phi ( x ) )
	\end{equation}
	pushes \( \mu \) forward to \( v \). This map is uniquely characterized among all maps pushing \( \mu \) forward \( v \) by formula \cref{equa:transform_map} with \( \phi \in \mathcal{I} ^ { c } ( X , Y ) . \) Furthermore \( F \) is the unique minimizer of the quadratic cost \( \int d ^ { 2 } ( x , G ( x ) ) d \mu ( x ) \) among all Borel maps \( G : M \rightarrow M \) pushing \( \mu \) forward to \( v \) (apart from variations on sets of \( \mu \)-measure zero).
\end{thm}

The map \( F \) may be referred to either as the optimal map or optimal mass
transport between \( \mu \) and \( v \).

Let us also recall one of the basic lemmas from its proof, which illumi-
nates the structure of the map \( F \). Given two compact subsets \( X \) and \( Y \subset M \)
with \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \), one sees every \( ( x , y ) \in X \times Y \) satisfy
\begin{equation}
	\label{equa:c-concave_conjugate}
	c ( x , y ) - \phi ( x ) - \phi ^ { c } ( y ) \geq 0
\end{equation}
with equality when \( \phi ( x ) = \inf _ { y ^ { \prime } \in Y } c \left( x , y ^ { \prime } \right) - \phi ^ { c } \left( y ^ { \prime } \right) = c ( x , y ) - \phi ^ { c } ( y ) . \)

\begin{lem}
	[Elementary properties of \( c \) -concave functions]
	\label{lem:minimizer_differentiable}
	Fix \( x \subset \subset M \) open and \( Y \subset M \) compact. For \( \phi \in I ^ { c } ( \bar { X } , Y ) \) define \( F ( x ) : = \)
	\( \exp _ { x } ( - \nabla \phi ( x ) ) \).
	\begin{enumerate}
		\item The function \( \phi \) is Lipschitz on \( \bar { X } \) and hence differentiable almost every-
		      where on \( X \).
		\item Fix any point \( x \in X \) where \( \phi \) is differentiable. Then \( y = F ( x ) \) if and
		      only if \( y \) minimizes \cref{equa:c-concave_conjugate} among \( y ^ { \prime } \in Y . \) In the latter case one has
		      \( \nabla \phi ( x ) = \nabla d _ { y } ^ { 2 } ( x ) / 2 \).
	\end{enumerate}
\end{lem}

\begin{defn}
	[$c$-superdifferential \( \partial ^ { c } \phi \)]
	Let \( X , Y \) be two compact sets of M. For \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \)
	and \( x \in X \), the \( c \)-superdifferential of \( \phi \) at \( x \) is the non-empty set
	\begin{align}
		\partial ^ { c } \phi ( x ) & : = \left\{ y \in Y \mid \phi ( x ) + \phi ^ { c } ( y ) = c ( x , y ) \right\}                   \\
		                            & = \{ y \in Y \mid \phi ( z ) \leq \phi ( x ) + c ( z , y ) - c ( x , y ) \quad \forall z \in X \}
		\label{equa:c-superdifferential}
	\end{align}
\end{defn}

\begin{example} [Multivalued extension]
	\label{example:minimizer_differentiable}
	If \( \phi \in \mathcal { I } ^ { c } ( \bar { X } , Y ) \) is differentiable at
	\( x \in \mathcal { X } \subset \subset \),
	then \( \partial ^ { c } \phi ( x ) = \{ F ( x ) \} = \left\{ \exp _ { x } ( - \nabla \phi ( x ) ) \right\} \) according to \cref{lem:minimizer_differentiable}.
\end{example}

First recall that a geodesic ball \( B _ { r } ( x ) \) of radius \( r \) around \( x \in M \) is said to be embedded if the exponential map \( \exp _ { x } : \tilde { B } _ { r } ^ { x } ( 0 ) \rightarrow B _ { r } ( x ) \) defines a diffeomorphism from the open ball \( \tilde { B } _ { r } ^ { x } ( 0 ) \subset T _ { x } M \) onto \( B _ { r } ( x ) \subset M \).
A geodesic ball \( B _ { r } ( x ) \) around \( x \) is a convex embedded ball if it is embedded and geodesically convex---meaning every pair of points \( y , z \in B _ { r } ( x ) \) are joined by a unique geodesic of length less than \( 2 r \), and this geodesic is contained in \( B _ { r } ( x )\).
Small enough balls are always convex embedded balls.

\begin{defn}[Semi-concavity]
	\label{defn:semi-concavity}
	Fix \( \Omega \subset M \) open. A function \( \phi : \Omega \rightarrow \mathbf { R } \) is semi-concave at \( x _ { 0 } \in \Omega \) if there exists a convex embedded ball \( B _ { r } \left( x _ { 0 } \right) \) and a smooth function \( V : B _ { r } \left( x _ { 0 } \right) \rightarrow \mathbf { R } \) such that \( \phi + V \) is geodesically concave throughout \( B _ { r } \left( x _ { 0 } \right) . \) The function \( \phi \) is semi-concave on \( \Omega \) if it is semi-concave at each point of \( \Omega \).
\end{defn}

\begin{defn}[Hessian]
	Let \( \phi : \Omega \rightarrow \mathbf { R } \) be semi-concave on an open set \( \Omega \subset M . \) We say that \( \phi \) has \( a \) Hessian \( H \) at \( x \in \Omega \) if \( \phi \) is differentiable at \( x \)
	and there exists a self-adjoint operator \( H : T _ { x } M \rightarrow T _ { x } M \) satisfying
	\begin{equation}
		\label{defn:hessian}
		\sup _ { v \in \partial \phi \left( \exp _ { x } u \right) } \left| \Pi _ { x , u } v - \nabla \phi ( x ) - H u \right| = o ( | u | )
	\end{equation}
	as \( u \rightarrow 0 \) in \( T _ { x } \) M. Here \( \Pi _ { x , u } : T _ { \exp _ { x } u } M \rightarrow T _ { x } M \) denotes parallel translation to \( x \) along \( \gamma ( t ) : = \exp _ { x } ( t u ) . \) The Hessian of \( \phi \) at \( x \) may also be denoted
	by \( \operatorname { Hess } _ { x } \phi : = H \).
\end{defn}

This definition coincides with the usual one for smooth functions. A more intuitive understanding of the Hessian follows from the fact that existence of a Hessian \( H \) at \( x \) for \( \phi \) implies a second order Taylor expansion for \( \phi \) around \( x : \) as \( u \rightarrow 0 \in T _ { x } M \),
\begin{equation}
	\label{equa:hessian_expan}
	\phi \left( \exp _ { x } u \right) = \phi ( x ) + \langle \nabla \phi ( x ) , u \rangle + \frac { 1 } { 2 } \langle H u , u \rangle + o \left( | u | ^ { 2 } \right)
\end{equation}

It is remarkable that the converse also holds true: if \( \psi \) is semi-concave around \( x \) then \cref{defn:hessian} follows from \cref{equa:hessian_expan}.

\begin{prop}[\( c \)-concave functions are semi-concave]
	\( F i x X \subset \subset M \) open and \( Y \subset M \) compact. A \( c \)-concave function \( \phi \in \mathcal{I} ^ { c } ( \bar { X } , Y ) \) is semi-concave on \( X \) (and hence admits a Hessian \cref{defn:hessian} almost everywhere in \( X \)).
\end{prop}

\begin{thm}[Aleksandrov-Bangert, \cite{bangert1979analytiche} in German]
	Let \( \phi : \Omega \rightarrow M \) be semi-concave function on an open set \( \Omega \subset M . \) Then \( \phi \) admits a Hessian almost everywhere on \( \Omega \).
\end{thm}

This theorem is also proved as Theorem 14.1 in \cite{villani2008optimal}.
The notion of local semiconvexity
with quadratic modulus is invariant by $C^2$ diffeomorphism, so it suffices to prove for $\mathbb{R}^n$.


\begin{prop} [Differentiating optimal transport]
	\label{prop:differentiate_optimal_transport}
	Fix \( X \subset \subset M \) be openand \( Y \subset M \) compact. Let \( \phi \in I ^ { c } ( \bar { X } , Y ) \) and set \( F ( z ) : = \exp _ { z } ( - \nabla \phi ( z ) ) . \)
	Fix a point \( x \in X \) where \( \phi \) admits a Hessian \cref{defn:hessian}.
	Then:
	\begin{enumerate}
		\item \( y : = F ( x ) \notin \operatorname { cut } ( x ) \) and setting \( H : = \operatorname { Hess } _ { x } d _ { y } ^ { 2 } / 2 \), one has \( H - \operatorname { Hess } _ { x } \phi \) \( \geq 0 \)
		\item Introduce \( Y : = d \left( \exp _ { x } \right) _ { - \nabla \phi ( x ) } \) and define \( d F _ { x } : T _ { x } M \longrightarrow T _ { y } M \) by
\( d F _ { x } : = Y \left( H - \operatorname { Hess } _ { x } \phi \right) \). Then as \( u \rightarrow 0 \) in \( T _ { x } M \),
\begin{equation} 
	\sup _ {\substack {\exp _ { y } v \in \partial ^ { c } \phi \left( \exp _ { x } u \right) \\ | v | = d \left( y , \exp _ { y } v \right)} } \left| v - d F _ { x } ( u ) \right| = o ( | u | ) 
\end{equation}

	\end{enumerate}
\end{prop}

We only copy proof for the first conclusion, see Proposition 4.1 in \cite{cordero2001riemannian} for full detail.
\begin{proof}
	Suppose \( \phi \) admits a Hessian \cref{defn:hessian} at \( x \in X \).
	Then \( \phi \) is differentiable at \( x \) and
	\cref{example:minimizer_differentiable} shows that \( \partial ^ { c } \phi ( x ) = \{ F ( x ) \} = \{ y \} \).
	Thus for every \( z \in \mathcal { X } \), \cref{equa:c-superdifferential} yields
	\begin{equation}
		\label{equa:c-concave_distance_compare}
		\phi ( z ) \leq \phi ( x ) + d _ { y } ^ { 2 } ( z ) / 2 - d _ { y } ^ { 2 } ( x ) / 2 \end{equation}
	Taking \( z = \exp _ { x } ( \pm u ) \) and \( \psi : = d _ { y } ^ { 2 } / 2 \) gives
	\[ \frac { \phi \left( \exp _ { x } u \right) + \phi \left( \exp _ { x } - u \right) - 2 \phi ( x ) } { | u | ^ { 2 } } \leq \frac { \psi \left( \exp _ { x } u \right) + \psi \left( \exp _ { x } - u \right) - 2 \psi ( x ) } { | u | ^ { 2 } } \]
	As \( | u | \rightarrow 0 \) the left hand side tends to \( \left\langle \operatorname { Hess } _ { x } \phi ( u ) , u \right\rangle \) by hypothesis, so the right hand side is bounded below.
	Then \cref{prop:distance_cut_locus} ensures that
	\( x \notin \operatorname { cut } ( y ) \), or equivalently \( y \notin c u t ( x ) \).
	From \cref{equa:c-concave_distance_compare} we also observe that the function
	\[ h ( z ) : = d _ { y } ^ { 2 } ( z ) / 2 - \phi ( z ) \]
	has a minimum at \( z = x \). The Taylor expansion \cref{equa:hessian_expan} then implies the
	existence and non-negativity of its Hessian: Hess\(_{ x } h = H - \) Hess\(_ { x } \phi \geq 0\).
\end{proof}

From the proof of Lemma 3.11 in \cite{cordero2001riemannian}, one can actually choose the local smooth function in \cref{defn:semi-concavity} as square distance function.
\section{Paper review}

This is review of \cite{KIM2017640},
Wasserstein barycenters over Riemannian manifolds.

\subsection{Remind of basics in Riemannian geometry}

For \( c ( x , y ) = \frac{ 1 } { 2 } d ^ { 2 } ( x , y ) \), to show \( - D _ { x } c ( x , y ) = \exp _ { x } ^ { - 1 } ( y ) \), we should use exponetial coordinate at $T_yM$.
By Gauss lemma, we have \( \nabla r = \partial _ { r } \).
As $d c = r dr$, $\nabla c = r \partial r$ and our conculsion follows from that $ r \partial r $ is of length $r$.

It is instructive to recall proof of Gauss lemma in \cite{Petersen2016}.
On \( U = \exp _ { p } ( B ( 0 , \varepsilon ) ) \) we define the function \( r ( x ) = \left| \exp _ { p } ^ { - 1 } ( x ) \right| . \)
That is, \( r \) is simply the Euclidean distance function from the origin on \( B ( 0 , \varepsilon ) \subset T _ { p } M \) in exponential coordinates.
This function can be continuously extended to \( U \) by defining \( r ( \partial U ) = \varepsilon . \)

We know that \( \nabla r = \partial _ { r } = \frac { 1 } { r } x ^ { i } \partial _ { i } \) in Cartesian coordinates
on \( T _ { p } M . \)
We show that this is also the gradient with respect to the general metric \( g . \)

\begin{lem}
	[The Gauss Lemma]
	On \( ( U , g ) \) the function \( r \) has gradient \( \nabla r = \partial _ { r } \), where \( \partial _ { r } = D \exp _ { p } \left( \partial _ { r } \right) \).
\end{lem}

\begin{proof}
	We select an orthonormal basis for \( T _ { p } M \) and introduce
	Cartesian coordinates.
	These coordinates are then also used on \( U \) via the exponential map.
	Denote these coordinates by \( \left( x ^ { 1 } , \ldots , x ^ { n } \right) \) and the coordinate vector fields by
	\( \partial _ { 1 } , \ldots , \partial _ { n } . \)
	Then
	\begin{align*}
		r ^ { 2 }        & = \left( x ^ { 1 } \right) ^ { 2 } + \cdots + \left( x ^ { n } \right) ^ { 2 } , \\
		\partial _ { r } & = \frac { 1 } { r } x ^ { i } \partial _ { i }.
	\end{align*}
	For this, take a function $f: M \mapsto \mathbb{R}$, we have $ \frac{\partial f}{\partial x_i}=\frac{\partial f}{\partial r}\cdot \frac{\partial r}{\partial x_i}$.
	Differentiate $ r ^ { 2 } = \left( x ^ { 1 } \right) ^ { 2 } + \cdots + \left( x ^ { n } \right) ^ { 2 } $, we get $\frac{\partial r}{\partial x_i} = \frac{x_i}{r} $.
	Apply this equality agian, we can solve $\frac{\partial f}{\partial r}$ from $\frac{\partial f}{\partial x_i}$.

	To show that this is the gradient field for \( r ( x ) \) on \( ( M , g ) \), we must prove that \( d r ( v ) = \)
	\( g \left( \partial _ { r } , v \right) . \)
	We already know that
	\[
		d r = \frac { 1 } { r } \left( x ^ { 1 } d x ^ { 1 } + \cdots + x ^ { n } d x ^ { n } \right),
	\]
	but have no knowledge of $g$, since it is just some abstract metric.

	One can show that \( d r ( v ) = g \left( \partial _ { r } , v \right) \) by using suitable Jacobi fields for \( r \) in place of \( v \). Let us start with \( v = \partial _ { r } . \)
	The right-hand side is 1 as the integral curves for \( \partial _ { r } \) are unit speed geodesics.
	The left-hand side can be computed directly and is also $1$.
	Next, take a rotational field \( J = - x ^ { i } \partial _ { j } + x ^ { j } \partial _ { i } , i , j = 1 , \ldots , n , i < j . \)
	In dimension $2$ this is simply the angular field \( \partial _ { \theta } \).
	An immediate calculation shows that the left-hand side vanishes: \( d r ( J ) = 0 \).
	For the right-hand side we first note that \( J \) really is a Jacobi field as \( L _ { \partial _ { r } } J = \left[ \partial _ { r } , J \right] = 0 . \)
	Using that \( \nabla _ { \partial _ { r } } \partial _ { r } = 0 \) we obtain
	\[ \begin{aligned}
			\partial _ { r } g \left( \partial _ { r } , J \right) & = g \left( \nabla _ { \partial _ { r } } \partial _ { r } , J \right) + g \left( \partial _ { r } , \nabla _ { \partial _ { r } } J \right) \\
			                                                       & = 0 + g \left( \partial _ { r } , \nabla _ { \partial _ { r } } J \right)                                                                   \\
			                                                       & = g \left( \partial _ { r } , \nabla _ { J } \partial _ { r } \right)                                                                       \\
			                                                       & = \frac { 1 } { 2 } D _ { J } g \left( \partial _ { r } , \partial _ { r } \right)                                                          \\
			                                                       & = 0
		\end{aligned} \]
	Thus \( g \left( \partial _ { r } , J \right) \) is constant along geodesics emanating from \( p \).
	To show that it vanishes first observe that
	\[ \begin{aligned} \left| g \left( \partial _ { r } , J \right) \right| & \leq \left| \partial _ { r } \right| | J |                                                                               \\
                                                                     & = | J |                                                                                                                  \\
                                                                     & \leq \left| x ^ { i } \right| \left| \partial _ { j } \right| + \left| x ^ { j } \right| \left| \partial _ { i } \right| \\
                                                                     & \leq r ( x ) \left( \left| \partial _ { i } \right| + \left| \partial _ { j } \right| \right)\end{aligned} \]
	Continuity of \( D \exp _ { p } \) shows that \( \partial _ { i } , \partial _ { j } \) are bounded near \( p .  \)
	Thus \( \left| g \left( \partial _ { r } , J \right) \right| \rightarrow 0 \) as \( r \rightarrow 0 .  \)
	This forces \( g \left( \partial _ { r } , J \right) = 0 .\)
	Finally, observe that any vector \( v \) is a linear combination of \( \partial _ { r } \) and rotational fields.
	This proves the claim.
\end{proof}

\subsection{Fix typos and correct statements}

We copy original statement and put reference number directly after it.

\begin{rmk}[Remark 2.2]
	Inspection of the proof above shows that \( ( M \), vol \( ) \) can be replaced with a \cancel{compact separable} metric space \( ( X , \nu ) \) equipped with a reference Borel measure \( \nu \).
\end{rmk}

We only need an outer regular reference measure. And Borel measure on metric space is regular, see Theorem 7.1.7 in \cite{Bogachev2007}.

\begin{prop}[Proposition 2.9 Distortion under Ric \( \geq 0 \)]
	Suppose the Ricci curvature of \( M \) is everywhere nonnegative, i.e., Ric \( \geq 0 . \) Then, for any \( x \in M \) and \( \lambda \in P ( M ) \), \textcolor{blue}{if $\lambda$ gives no mass to the cut-locus of its baryceter $\bar{x}$}, we have
	\[ \alpha _ { \lambda } ( x ) \geq 1 \]
\end{prop}

\begin{proof}[Proof of Prop 2.9]
	Minimality of \( z \mapsto \int _ { M } c ( x , z ) \diff \lambda ( x ) \) at the barycenter \( \bar { x } \), combined with semi-concavity of \( z \mapsto c ( x , z ) \) and Fatou's lemma yields
	\[
		\int _ { M } \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } c ( x , z ) \diff \lambda ( x ) \geq 0
	\] as a matrix.
\end{proof}

Locally $c(x,z)=:d^2_x(z)$ near $\bar{x}$ is a geodesically convex function, by linear intergration so is the integral  \( z \mapsto \int _ { M } d^2_x(z) \diff \lambda ( x ) \).
As convexity implies local Lipschitz, $d^2_x(z)$ is also differentiable near $\bar{x}$ and so is its integral by compactness of $M$.
Thus we have locally for $\lambda$-almost all $x$ (not in the cut-locus of $\bar{x}$, arguing with semi-concavity is not strong as this),
% \textcolor{red}{why differentiability of $d^2_x(z)$ implies smoothness?}
\begin{equation}
	\label{equa:convex_distance_inequality}
	d^2_x \left( \exp _ {\bar{x}} u \right) - d^2_x (\bar{x}) - \langle \nabla d^2_x (\bar{x}) , u \rangle = \frac { 1 } { 2 } \langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z) \, u , u \rangle + o \left( | u | ^ { 2 } \right) > 0
\end{equation}

To show positivity of matrix, we take a vector $\nu$ and set $\mu = h \nu$ with $h > 0$ small enough.
Divide by $h$ in \cref{equa:convex_distance_inequality} and then take integral,
\[
	\langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } \int_{M} d^2_x(z) \diff \lambda ( x )\, v , v \rangle + \frac{o (h^2)}{h^2} = \int_{M} \langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z)\, v , v \rangle + \frac{o (h^2)}{h^2} \diff \lambda ( x ).
\]
Note that here we need compactness of $M$, hence boundness of $ \nabla d^2_x(\bar{x}) $ for $x \in M$, to get inter-change of integral and gradient:
\[
	\int_M \langle \nabla d^2_x (\bar{x}) , u \rangle \diff \lambda (x) = \langle \int_M \nabla d^2_x (\bar{x}) \diff \lambda (x) , u \rangle
\]
Apply Fatou's lemma, let $f\downarrow 0$,
\[
	\langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } \int_{M} d^2_x(z) \diff \lambda ( x )\, v , v \rangle \leq \langle \int_{M} \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z) \diff \lambda(x)\, v, v \rangle.
\]
And by minimallity of $\bar{x}$, we know left hand side is non-negative.

This argument is also used in Prop 4.2.

\begin{rmk}[Remark 3.2]
	... In fact, it holds for any (compact) metric
	space on which the optimal \cancel{maps} \textcolor{blue}{plans}, \( T _ { \# } \mu = \nu \), exist uniquely for any arbitrary absolutely continuous source measure \( \mu \).
\end{rmk}

\begin{lem}[Lemma  4.1  a.e. \( x \) and $ \Omega$-a.e. $\mu$ ]
	Let \( \bar { \mu } \in \cancel{P ( M )} \textcolor{blue}{P_{ac}(M)}\) and for each \( \mu \in P ( M ) \), let \( u _ { \mu } \) be the dual potential (whose gradient is uniquely determined \( \bar { \mu } \) almost everywhere) for the optimal transport problem between \( \bar { \mu } \) and \( \mu . \) Let \( \Omega \) be a Borel probability on \( P ( M ) . \) For volume almost all \( x , x \mapsto u _ { \mu } ( x ) \) is twice differentiable for \( \Omega \) -almost all \( \mu \in P ( M ) . \)
\end{lem}

\begin{proof}
	$\forall \mu, x \mapsto \mu_u(x)$ is twice differentiable for $\bar{\mu}$-a.e. $x$. Apply Fubini's theorem then.
\end{proof}

\begin{prop} [Prop 4.2 Derivatives inside the integral $\int _ { P ( M ) } \diff \Omega$]
	\begin{equation}
		\label{equa:first_order}
		\nabla _ { x } \int _ { P ( M ) } u _ { \mu } ( x ) \diff \Omega ( \mu ) = \int _ { P ( M ) } \nabla _ { x } u _ { \mu } ( x ) \diff \Omega ( \mu )
	\end{equation}
	\begin{equation}
		\label{equa:second_order}
		\nabla _ { x } ^ { 2 } \int _ { P ( M ) } u _ { \mu } ( x ) \diff \Omega ( \mu ) \geq \int _ { P ( M ) } \nabla _ { x } ^ { 2 } u _ { \mu } ( x ) \diff \Omega ( \mu )
	\end{equation}
\end{prop}

\begin{proof}
	This can be seen by applying the dominated convergence theorem for \cref{equa:first_order} due to uniform Lipschitzness \textcolor{blue}{by \cref{lem:infimal_convolution_Lipschitz}} of \( u _ { \mu } \) and Fatou's lemma for \cref{equa:second_order} due to the semi-convexity
	of \( u _ { \mu } . \)
\end{proof}

% This shouldn't be an independent proposition, rather we can only condsider it in the case of Theorem 4.4.
% $u_{\mu}$ is uniformly Lipschitz on $x\in M$, this shows that $\nabla_{x}u_{\mu}$ is finite but not necessarily bounded (dominated).
To get rid of measurable selection problem, we consider
\[ y \mapsto \int_{P(M)} c(y, T_{\mu}(x)) \diff \Omega ( \mu)\]
at point $x$.
Here we assume $\bar{\mu}$ is barycenter of $\Omega$ and $T_{\mu}$ is the transfer map from $\bar{\mu}$ to $\mu$.
Then this integral valued map is well-defined $\mu$-a.e. by Fubini's theorem.
By stability of Kantorobich potentials (Theorem 1.52 in \cite{Santambrogio2015}), $c(y, T_{\mu}(x))$ is continous with respect to $\mu \in P(M)$.
Fix a $\mu$, $c(y, T_{\mu}(x))$ is locally Lipschitz with respect to $y$.
By continuity Lipschitz inequality will hold as well in a neighbourhood of $\mu$.
Consider these local neighbourhoods cover on compact set $ M \times P(M)$,
we can then get a uniform boundness on
\[\left. D_y\right|_{y=x} c(y, T_{\mu}(x)) = - \nabla_x u_{\mu}(x).\]


\begin{lem} [Lem 4.3 Riemannian barycenter from Wasserstein barycenter]
	\label{lem:inverse_barycenter}
	Let \( \bar { \mu } \) be a Wasserstein barycenter of the measure \( \Omega \) on \( P ( M ) \) and assume \( \bar { \mu } \) is absolutely continuous with respect to volume;
	let \( T _ { \mu } \) be an optimal map from \( \bar { \mu } \) to \( \mu . \)
	Let \( \lambda _ { z } = \left( \mu \mapsto T _ { \mu } ( z ) \right) _ { \# } \Omega . \)
	Then, for \( \bar { \mu } \) almost every \( z , z \) is a barycenter of \( \lambda _ { z }\).
	\textcolor{blue}{And $\lambda_z$ gives no mass to the cut-locus of $z$} by \cref{prop:differentiate_optimal_transport}.

	If, in addition, \( \Omega \left( P _ { a c } ( M ) \right) > 0 \), then for \( \bar { \mu } \) almost every \( z , z \) is the unique barycenter of $\lambda _ { z }$.

\end{lem}

There is an explanation behind this lemma. Consider we have random images, and we want to approch a best representative of them by simulation. We process this by compose all barycenters of simulated images into an average image. On the other hand, for each grid point in our average image, we can simulate a new image by transfering that grid point optimally. This lemma claims that that choosen grid point should be a barycenter of the new generated image. This could be related to ergodic theory as there are two kinds of averages involved.


\begin{proof}[Proof of the 1st order balance]
	...
	% On the other hand, by Lemma  4.3, for \( \bar { \mu } \) almost every \( x \), we have that \( x \) is the
	% barycenter of \( \lambda _ { z } = \left( \mu \mapsto T _ { \mu } ( z ) \right) _ { \# } \Omega ; \) that is, a minimizer of
	% \[ f _ { x } : y \mapsto \int _ { P ( M ) } d ^ { 2 } \left( y , T _ { \mu } ( x ) \right) d \Omega ( \mu ) .\]

	Therefore, the latter function \( f _ { x } \), which is semi-concave is differentiable at \( x : \) due to semi-concavity, there is \( C > 0 \) such that the function \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) is locally geodesically concave near \( x \).
	Minimality at \( x \) implies \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \geq f _ { x } ( x ) - \) \( C \operatorname { dist } ^ { 2 } ( x , \cancel{x}\, \textcolor{blue}{y} ) \).
	Since \( y \mapsto f _ { x } ( x ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) has vanishing derivative at \( x \), concavity of \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) implies that locally the function \( y \mapsto f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) is also
	locally bounded from above by the constant \( f _ { x } ( x ) . \) This implies the differentiability of
	\( f _ { x } \) at \( x \).
\end{proof}

% \textcolor{cyan}{If we have} that $\lambda_{z}$ is absolutely continous, then we prove first order balance by taking gradient under integral.
% For the sencond paragraph of argument,
To simplify, we should prove:
\begin{lem}
	For $f, g$ two convex functions near $0$, assume $f(0)=g(0)=g^\prime(0)=0$. If $f \leq g$ and $0$ is a local minimum of $g$, we then have $f \geq 0$ and $f$ is differentiable at $0$.
\end{lem}

\begin{proof}
	We need to show that subdifferential $\partial f(0) $ of $f$ is a single point set $ \{ 0 \}$. $\forall u \in \partial f( 0 )$,
	\[
		\langle x, u \rangle \leq f(x) - f(0) = f(x) \leq g(x) = g(x) - g(0),
	\]
	that is say, $ u \in \partial g(0)$. Hence $ \partial f(0) = \partial g(0) = \{0\}$.

\end{proof}

\begin{proof}[Proof of Theorem 4.6]
	...\\
	Note that each \( - D _ { x y } ^ { 2 } c \left( x , T _ { \mu } ( x ) \right) D T _ { \mu } ( x ) = D _ { x x } ^ { 2 } u _ { \mu } ( x ) + D _ { x x } ^ { 2 } c \left( x , T _ { \mu } ( x ) \right) \) is positive semi-definite by the \( c \)-convexity of \( u _ { \mu } \), and hence so is their integral...
\end{proof}

By definition, we have
\[
	\inf_{\textcolor{cyan}{z}} u_\mu(\textcolor{green}{z}) + c(\textcolor{green}{z}, T_\mu(x))= u_\mu(x) + c(x, T_\mu(x)) = - u^c ( T_\mu(x)).
\]
If consider derivative with respect to first variable,
then we have first differential vanishes and Hessian semi-positive.


% \section{Multi-marginal optimal transport on Riemannian manifolds}
% We will prove uniqueness and Monge solution results for the multi-marginal problem on a compact Riemannian manifold,
% with cost function
% \begin{equation}
% 	\label{equa:mult-imarginal_problem}
% 	c \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) = \inf _ { y \in M } \sum _ { i = 1 } ^ { m } \frac { d ^ { 2 } } { 2 } \left( x _ { i } , y \right)
% \end{equation}

% \begin{lem}
% 	Fix \( \left( x _ { 1 } , \ldots , x _ { m } \right)\).
% 	Then any \( y \) which minimizes \( y \mapsto \sum _ { i = 1 } ^ { m } d ^ { 2 } \left( x _ { i } , y \right) \) is not in the cut locus of \( x _ { i } \) for any \( i . \)
% \end{lem}

% \begin{proof}
% 	Choose a point \( y \) in the cut locus of \( x _ { i } \) for some \( i \);
% 	we will show that \( y \) cannot minimize \( y \mapsto \sum _ { i = 1 } ^ { m } d ^ { 2 } \left( x _ { i } , y \right) . \)
% 	By Lemma 3.12 in \cite{cordero2001riemannian}, we can find a constant \( K \) such that, for all \( u \in T _ { y } M \), and \( j = 1,2 , \ldots m \), we have
% 	\[ \frac { d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) - 2 d ^ { 2 } \left( x _ { j } , y \right) } { | u | ^ { 2 } } \leq K \]
% 	On the other hand, by Proposition \( 2.5 \) in the same paper, we can find some non
% 	zero \( u \in T _ { y } M \) such that
% 	\[ \frac { d ^ { 2 } \left( x _ { i } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { i } , \exp _ { y } ( - u ) \right) - 2 d ^ { 2 } \left( x _ { i } , y \right) } { | u | ^ { 2 } } \leq - m K \]
% 	Therefore, we have
% 	\[ \begin{aligned} \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) & = \sum _ { j \neq i } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) + d ^ { 2 } \left( x _ { i } , y \right) \\ & \geq \frac { - ( m - 1 ) K | u | ^ { 2 } } { 2 } + \frac { 1 } { 2 } \sum _ { j \neq i } ^ { m } \left( d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) \right) \\ & + \frac { m K | u | ^ { 2 } } { 2 } + \frac { 1 } { 2 } d ^ { 2 } \left( x _ { i } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { i } , \exp _ { y } ( - u ) \right) \\ & > \frac { 1 } { 2 } \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + \frac { 1 } { 2 } \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) \end{aligned} \]
% 	Therefore, either
% 	\[ \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) < \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \]
% 	or
% 	\[ \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } - u \right) < \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \]
% 	in either case, \( y \) cannot minimize \( \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \)
% \end{proof}

% \begin{lem}
% 	The cost function \( c \) is everywhere superdifferentiable with respect to \( x _ { 1 } \).
% 	That is, for all \( \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) \in M ^ { m } \) there exist \( p \in T _ { x _ { 1 } } M \) (the super-gradient) such that, for small \( v \in T _ { x _ { 1 } } M \), we have
% 	\[ c \left( \exp _ { x _ { 1 } } v , x _ { 2 } , \ldots , x _ { m } \right) \leq c \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) + g ( p , v ) + o ( | v | ) \]
% \end{lem}

% \begin{lem}
% 	At any point \( \left( x _ { 1 } , \ldots , x _ { m } \right) \) where \( c \) is differentiable with respect to
% 	\( x _ { 1 } \), there is a unique minimizing \( y \) in \cref{equa:mult-imarginal_problem}, and moreover,
% 	\[ y = \exp _ { x _ { 1 } } \left( \nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) \right) \]
% \end{lem}

% \begin{proof}
% 	For any minimizing \( y \) in \cref{equa:mult-imarginal_problem},
% 	\( d ^ { 2 } \left( x _ { 1 } , y \right) \) is differentiable as \( y \notin \operatorname { cut } \left( x _ { 1 } \right) \).
% 	We then have
% 	\[
% 		\nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) = \nabla _ { x _ { 1 } } \left( \frac { 1 } { 2 } d ^ { 2 } \left( x _ { 1 } , y \right) \right)
% 	\]
% 	This equation implies that \( y \) must equal \( \exp _ { x _ { 1 } } \left( \nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) \right) \);
% 	uniqueness follows immediately.
% \end{proof}


\section{Barycenter on manifold}

% \subsubsection{A perspective from optimal transportation}

% Make use of previous discussions, we have following nice property

% \textcolor{red}{ALL THESE ARE WRONG! So I finally change orignal $i$ into $j$.
% That is to say, we fix $i$ in following lemma, everything is so trivial now.}

% \begin{lem}
% Let $\mu_j, j=1,2,\ldots,n$ be a $n$ measures on compact Riemanninan manifold $M$.
% Assume $\mu_i$ absolutely continuous, and other $\mu_j, j \neq i$ are discrete.
% % absolutely continous with respect to volume measure on $M$.
% Then the measure $\sum_{j=1}^n \lambda_j \delta_{\mu_j} \in P(P(M))$ has a unique barycenter $\bar{\mu}$ on $M$.
% $\bar{\mu}$ is absolutely continous with respect to volume measure on $M$.
For $n$ points $x_i \in M, i=1,2,\ldots,n$, we define a function on $M^n$,
\[
	f(x_1, x_2, \ldots, x_n) := \min_{z} \sum_{i=1}^n \lambda_i c(z, x_i)^2 =: \frac{1}{2}W^2(\sum_{i=1}^n \lambda_i \delta_{x_i}, M).
\]
Note that $f_i / \lambda_i $ is a $c$-concave function with a global Lipschitz constant bounded by diameter of $M$.
This conclusion relies heavily on compactness of manifold $M$.

% \end{lem}
Denote by $B$ a measurable selection of barycenter of $n$-points supported measures on $M$,
\[
	B(\boldsymbol{x}) \in \arg \min_{x \in M} \sum_{i=1}^{n} \lambda_i d(x, x_i)^p.
\]
% Let $T_i$ be the unique optimal transfer map from $\bar{\mu}$ to $\mu_i$,
% and define $\gamma := (T_1, T_2, \ldots, T_n)_{\#} \bar{\mu}$ to be the push-forward measure of $\bar{\mu}$ on $M^n$.

% Then for $(x_1, x_2, \ldots, x_n)$ in the support of $\gamma$,
% there is $\gamma$-a.e. a unique barycenter $B(x_1, x_2, \ldots, x_n)$ of measure $\sum_{j=1}^n \lambda_j \delta_{x_j}$.
% Both map $B$ and $f$ are $\gamma$-a.e. differentiable.
% The map $B(T_1, T_2, \ldots, T_n)$ is $\bar{\mu}$-a.e. the identity map on $M$.

We have following barycenter formula holds almost everywhere for volume measure,
% if $\mu_i$ is absolutey continous
\begin{equation}
	\label{formula_barycenter}
	B = \exp (- \frac{1}{\lambda_i} \nabla_i f),
\end{equation}
where $\nabla_i f$ means partial gradient of $f$,
i.e., $\nabla_i f = \nabla f_i$ if we define $f_i$ as function on $M$ through $f$ by fixing $x_j$ for $j \neq i$.

% \begin{proof}
% We intend to apply discussion of conditional probability in \cref{discussion_conditional_prob} for the case
% that $\gamma$ is absolutely continous.
% Firstly we should prove that $\gamma$ is absolutely continous,
% and we may get inspiration from \cite{santambrogio2009absolute} for the case of $n=2$.
% If this is done, we can apply previous discussion.

% \end{proof}

\begin{rmk}
	Some comments about this formula \cref{formula_barycenter}:
	\begin{itemize}
		\item The barycenter formula \cref{formula_barycenter} expresses the fact that,
		      from point $x_i$,
		      we can reach barycenter of $\sum_{j=1}^n \lambda_i \delta_{x_j}$ if we follow
		      the direction $-\nabla_i f$
		      and advance $\| \nabla_i f / \lambda_i \| $.
		\item In the case of Euclidean space, $\nabla_i (f/\lambda_i)(x_i)$ reduces to
		      $x_i - \sum_{j=1}^n \lambda_j x_j$.
		% \item $\exp(-\nabla f_i / \lambda_i )$ is the optimal transfer map from $\mu_i$ to $\bar{\mu}$.
		\item The $c$-conjugate function of $f_i / \lambda_i$ is $ (f_i / \lambda_i )^c(z) = -\sum_{j\neq i}^n \lambda_j c(z, x_j) / \lambda_i$.
		\item In the case of Eculidean space, $\nabla (f_i / \lambda_i)^c (z)$ reduces to

		      $(z - \sum_{j=1}^n \lambda_j x_j) / \lambda_i -(z-x_i)$.
	\end{itemize}
\end{rmk}

% \textcolor{red}{We have to fix $i$ and assume discrete measure for others,
% 	since first order balance holds in the case of Euclidean space only
% 	if $n=1$.}
% It is possible to get a inverse to $B$.
% without absolutely continuity of $\bar{\mu}$.
% To do so, we need to fix $n-1$ variables of $B$.
% We define a barycenter map from $M$ to $M$, 
% $br(x_1) := B(x_1, x^\prime) = \exp(-\frac{1}{\lambda_1}\nabla_1 f),$
% where $x^\prime := (x_2, x_3, \ldots, x_n)$;
% and we define
% $
% g ( z ) = -(f_1 / \lambda_1)^c=
% 	-\frac { 1 } { \lambda _ { 1 } } \sum _ { i = 2 } ^ { n } \lambda _ { i } \, c\left( x _ { i } , z \right),
% $
% Recall that for each \( x \),
% every \( z \in br\left( x , x _ { 2 } , \ldots . . x _ { n } \right) \) is not in the cut-locus of any \( x _ { i } \).
% Therefore, \( g ( z ) \) is twice differentiable at each \( z \in br\left( M , x _ { 2 } , \ldots , x _ { n } \right) . \)

% Now, for any point \( y \) such that
% \[ z \in br\left( y , x _ { 2 } , \ldots , x _ { n } \right) \]
% from the definition of \( g ( z ) \), we have \( \left. \nabla _ { w } \right| _ { w = z } c( y , w ) = - \nabla _ { z } g ( z ) \) or, equivalently,
% \( y = \exp _ { z } \nabla g ( z ) . \)
% Therefore, \( \exp _ { z } \nabla g ( z ) \) is the only point with the desired property.

Now we aim to prove that \( \exp(- \nabla (f_i /\lambda_i)^c) \) is a Lipschitz function with Lipschitz constant depending only on $M$ and $\lambda_i$.
As $M$ is compact, though tangent bundle $TM$ is not compact, local Lipschitz plus bounded diameter of $M$ implies global Lipschitz of $\exp$.
Function $(f_i /\lambda_i)^c$ has hessian upper bound as a $c$-concave function,
and has hessian lower bound as a negative linear combination of square distance function.
We then have
\begin{equation}
	\label{equa:hessian_bound_f}
	-\frac{1-\lambda_i}{\lambda_i} H \leq \nabla^2 \left(\frac{f_i}{\lambda_i}\right)^c \leq H,
\end{equation}
where we denote by $H$ the hessian of square distance funtion.
Recall that $H$ is bounded from above.
% Squared distance function $c$ has bounded Hessian from above.
% Hence $ \| \nabla^2 g \|$ is bounded from above by taking the second derivative of definition and also applying minimallity of cost at barycenter.

\section{Absolutely continuity of barycenter}

We discuss in the context of proof of \cref{thm:barycenter_finite_support_measure}.

\subsection{Barycenter of finite supported measure}

Consider a measure $\sum_{i}^{n} \lambda_{i} \delta_{\mu_i}$ on $P(M)$ for $M$ a compact Riemannian manifold.
Assume from now on $ \lambda_1 \neq 0$ and $\mu_1$ absolutely continous.

The barycenter $\bar{\mu}$ of measure $\sum_{i}^{n} \lambda_{i} \delta_{\mu_i}$ is unique,
because this meause gives mass to element in $P_{ac}(M)$.

If we \textcolor{cyan}{assume $\bar{\mu}$ is absolutely continous},
then there is only one element in the set $\Gamma$ by uniqueness of optimal plans.
We can get every $\mu_i$ from $\mu_1$ if first push $\mu_1$ to $\bar{\mu}$ through $T_1^{-1}$ then push $\bar{\mu}$ to $\mu_i$ through $T_i$. Hence,
\[B \circ (T_1, T_2, \ldots, T_n) \circ T_1^{-1} = T_1^{-1}\]
is the unique transfer map from $\mu_1$ to $\bar{\mu}$. That is to say, $\bar{\mu}$-a.e., $B \circ (T_1, T_2, \ldots, T_n) $ is the identity map.
This is already included in \cref{lem:inverse_barycenter}.

\subsubsection{One measure absolutely continous and others Dirac}
Here we \textcolor{cyan}{assume $\mu_i = \delta_{x_i}, i \geq 2$ are Dirac measures}.

As an inverse to $\exp(-\nabla f_1/\lambda_1)$,
\(\exp(-\nabla(f_1/\lambda_1)^c)\) pushes $\bar{\mu}$ to $\mu_1$.
% because $br$ pushes $\mu_1$ to $\bar{\mu}$ by construction of $\bar{\mu}$.
Denote by $C$ the Lipschitz constant of map \(\exp(-\nabla(f_1/\lambda_1)^c)\).
If we have $ \text{Vol}(E) < \delta \implies \mu_1(E) < \epsilon$,
then by $\text{Vol}(br^{-1}(E)) < C^n \text{Vol}(E)$
we have $\text{Vol}(E) < \delta / C^n \implies \bar{\mu}(E)=\mu_1(br^{-1}(E)) < \epsilon$.
Thus $\bar{\mu}$ is absolutely continous.

% \begin{rmk}
% 	Once after we prove that $\bar{\mu}$ is indeed absolutely continous.
% 	$\nabla g$ is invertable and $br(x_1) = \exp_{x_1} \nabla g^c(x_1)$.
% 	It is not surprised that
% 	\[
		% g^c(x_1) = -c(x_1 ,z) - \frac{1}{\lambda_1} \sum_{i=2}^{n} \lambda_i\, c(x_i, z) = 
		% - \frac{1}{2 \lambda_1} W^2( \sum_{i=1}^n \lambda_i \delta_{x_i}, M),
		% - f_1/\lambda_i.
	% \]
	% we can then take derivative with respect to $x_1$.
% \end{rmk}

\subsubsection{To a more general case by conditional probability}
% \label{discussion_conditional_prob}
To attack general case when \textcolor{cyan}{$\mu_i, i \geq 2$ are not assumed Dirac measures},
we should consider conditonal measures of optimal multi-marginal transfer plan $\gamma$:
\[
	\diff \gamma(x_1, x^\prime)= \gamma(\diff x_1 \mid x^\prime)\, \diff \pi(x^\prime) ,
\]
where $\pi$ is the projection of $\gamma$ from $x = (x_1, x^\prime)$ to $x^\prime$,
by abuse of language we also denote by it the push-forward measure from $\gamma$.
For $\pi$-a.e $x^\prime$., $\gamma(\cdot \mid x^\prime)$ is a probability measure
concentrated on $M \times \{x^\prime\}$.

One has from definition of conditional measures that
\[
	\bar{\mu} = B_{\#} \gamma = \int_{M^{n-1}} B_{\#} \gamma(\cdot \mid x^\prime)\, \diff \pi(x^\prime).
\]


If we have that for $\gamma$-a.e. $x^\prime$,
$B_{\#} \gamma(\cdot \mid x^\prime)$
is absolutely continous,
then $\bar{\mu}(E)=0$ for volume measure zero set $E$ by integration above.
% \[
% 	\bar{\mu}(E)
% =\int_{M^{n-1}}
% \frac{ \mu_1( br^{-1}(E) \cap A_{x^\prime} )}{\mu_1(A_{x^\prime})}
% \diff \pi(x^\prime)
% =\int_{M^{n-1}}
% \bar{\mu}^{x^\prime}(E)
% \diff \pi(x^\prime)
% \leq \int_{M^{n-1}} C\,\mu_1^{\prime}(E) \diff \pi(x^\prime) \leq C\, \mu_1(E).
% = 0.
% \]

Generally speaking, computation of $\gamma(\cdot \mid x^\prime)$ is only possible when
\begin{itemize}
	\item $\pi$ is has countable support.
	      This is equivalent to that $\mu_i, i \geq 2$ has countable support.
	      Fix any $x^\prime$ in the support of $\pi$, by direct verification
	      \[
		      \gamma(\cdot \mid x^\prime) =
		      \frac{
			      \mathbbm{1}_{M \times \{x^\prime\}}
			      \gamma
		      }{\pi(x^\prime)}.
	      \]
	\item $\gamma$ is absolutely continous with density function $f: M^n \rightarrow \mathbb{R} $.
	      \[
		      \gamma(\diff x_1 \mid x^\prime) =
		      \frac{
			      f(x_1, x^\prime) \diff \text{Vol}(x_1)
			      % \diff \gamma (x_1, x^\prime)
		      }
		      {\int_{M} f(x_1, x^\prime) \diff \text{Vol}(x_1)
		      }
	      \]
	      where we set the right hand side zero if it is undeterminated
	      and we remove this $x^\prime$ from consideration.
\end{itemize}
Note in both cases,
$\gamma(\cdot \mid x^\prime)$
has absolutely continous push-forward measure
$\mu_1^{x^\prime} := \text{proj}^1_{\#}\gamma(\cdot \mid x^\prime)$
to the first coordinate.
But only in the first case that
measure $\gamma(\cdot \mid x^\prime) \leq \pi(x^\prime) \, \gamma$ is controled by $\gamma$.
Or we hope $f$ has positive lower bound,
for example \textcolor{cyan}{continous and strictly positive}, like volume measure.
In the case $\mu_1=\text{Vol}$? Every point in $\text{spt}\mu_1$ runs over all $\times_i \text{spt}\mu_i$.

\textcolor{red}{Unfortunately, this control is required to apply conditioning optimal plan.}
One may consider the case when at least one marginal is discrete,
but no absolutely continuity can be derived without barycenter push-forward.

It is true that we have $\gamma$ is an optimal plan for all its marginals
% \[
% 	B_{\#} \gamma              = \bar{\mu} \text{ and }
% 	\text{proj}^1_{\#} \gamma  = \mu_1,
% \]
and conditioning of (multi-marginal) optimal plan is still optimal from \cref{thm:restriction_optimal_plan}.
Hence, $\gamma(\cdot \mid x^\prime)$ is an optimal plan of its marginals.
% \textcolor{cyan}{in the case of discrete measure}.
Apply previous discussion, we get absolutely continous measure
\[
	\exp(-\frac{1}{\lambda_1}\nabla f_1)_{\#}  \mu_1^{x^\prime}=
	B_{\#}\gamma(\cdot, x^\prime) =
	\bar{\mu}^{x^\prime},
\]
where $\bar{\mu}^{x^\prime}$ is a barycenter measure.

Since conditioning of optimal maps are still optimal,
barycenter formula \cref{formula_barycenter} still holds.
We define $T_1 := \exp(-\nabla (f_1/\lambda_1)^c)$ to be the optimal transfer map from $\bar{\mu}^{x^\prime}$ to $\mu_1^{x^\prime}$,
\begin{align*}
	\bar{\mu} = B_{\#} \gamma & =
	\int_{M^{n-1}}\bar{\mu}^{x^\prime} \, \diff \pi(x^\prime)                         \\
	                          & = \int_{M^{n-1}}
	\det D T_1  \circ T_1^{-1}\, \gamma(\cdot \mid x^\prime) \diff \pi(x^\prime)      \\
	                          & = \det D T_1  \circ T_1^{-1}\, \gamma                 \\
	                          & = \exp (- \frac{1}{\lambda_1} \nabla_1 f)_{\#} \gamma
														.
\end{align*}
% One possible \textbf{investigation}: Conditional optimal plan, not just the simple case of restriction.
% Hence, the push-forward measure
% $\bar{\mu}^{x^\prime} = br_{\#} \diff \mu_1^{x^\prime}$ is absolutely continous as well.
% We thus have
% \[
% 	B_{\#} \diff \gamma = \int_{M^{n-1}} f(\cdot, x^\prime)  \diff \pi(x^\prime) \, \diff \text{Vol}(\cdot).
% \]
% Moreover, the previous integral is in fact a finite sum.
% For a measurable set $E \subset \text{spt}(\bar{\mu})$ with volume measure $0$,

% So $\bar{\mu} = B_{\#} \diff \gamma$ is absolutely continous with respect to $\mu_1$.
% \begin{rmk}
% 	\textcolor{red}{NEED further investigation}, maybe one can use \cref{formula_barycenter}.

% 	We may guess that	the density of $\bar{\mu}$,
% 	$\int_{M^{n-1}} f(\cdot, x^\prime)  \diff \pi(x^\prime)$
% 	is dominated by density of $\mu_1$ up to a constant coefficient.
% 	% Lipschitz constant of \( \exp _ { z } \nabla g ( z ) \).
% \end{rmk}

\subsubsection{To a more general case by consistency of barycenter}

Now we consider measure $\mathbb{P} = \lambda_1 \delta_{\mu_1} + \lambda_2 \delta_{\mu_2}$
without assuming $\mu_2$ discrete.
Note here it is for simplicity that we only consider the case $n=2$.
Approxiamte $\mu_2$ in Wasserstein metric by a sequence of measures $\mu_2^{m}$,
then $\mathbb{P}_m := \lambda_1 \delta_{\mu_1} + \lambda_2 \delta_{\mu_2^m}$ converges to $\mathbb{P}$
in $P(P(M))$.

By the consistency of barycenters, the unique barycenter $\bar{\mu}_m$ of $\mathbb{P}_m$
converges in Wasserstein metric (or just weakly) to the unique barycenter $\bar{\mu}$ of $\mathbb{P}$.
Recall that there is no duality between $L^{\infty}(\bar{\mu})$ (possible non-separable) and $L^1 (\bar{\mu})$
in functional analysis even when $M$ is compact with Lebesgue measure.
According to Proposition 4.4.2 in \cite{Bogachev2007} below, we need to show that $\bar{\mu}$ is a linear functional on $L^{\infty}(\bar{\mu})$.
\begin{prop}
	Let \( \mu \) be a finite nonnegative measure.
	A continuous linear function \( \Psi \) on \( L ^ { \infty } ( \mu ) \) has the form
	\( \Psi ( f ) = \int _ { X } f g d \mu \)
	, where \( g \in L ^ { 1 } ( \mu ) \),
	precisely when the set function \( A \mapsto \Psi \left( I _ { A } \right) \) is countably additive.
\end{prop}

We define $\Psi(f):= \int_M f \diff \bar{\mu} = \lim_m \int_M f \diff \bar{\mu}_m$ for $f$ continous.
Extend $\Psi$ to be a continous linear functional on $L^{\infty}(\bar{\mu})$ by Hahn-Banach theorem.
Do we still have $\Psi(I_A) = \bar{\mu}(A)$?

A related discussion is available on
\href{https://math.stackexchange.com/questions/574130/does-weak-convergence-with-uniformly-bounded-densities-imply-absolute-continuity/574888#574888}{math Stack Exchange}.
Think about this example carefully.
Let $\lambda$ be the arglength measure and $\phi_n \ge 0$ a continuous function on $\mathbb T$ with $\int_{\mathbb T} \phi_n\, d\lambda = 1$ and $\phi_n(x) = 0$ if $|x-1| \ge \frac 1n$. Then for each continuous function $f\colon \mathbb T \to \mathbb R$ we have $\int_{\mathbb T} f\phi_n d\lambda \to f(1)$, that is $\phi_n \lambda \to \delta_1$ weakly. But $\delta_1$ is not $\lambda$-continuous.

A digression, generally for question if density of $\bar{\mu}_m$ converges to density of $\bar{\mu}$,
we possibly need asymptotically equicontinuous in \cite{Sweeting1986Converse}.

We don't need to find out density explicitly.
Instead, let prove the absolutely continuity by showing that
for any $\epsilon > 0$ there is a $\delta > 0$ such that
\begin{equation}
	\label{equa:absolutely_continous}
	\forall E \subset M \, \text{measurable, } \text{Vol}(E) < \delta
	\implies \bar{\mu}(E) < \epsilon
\end{equation}
In our situation, a \textcolor{red}{uniform $\delta$} can be chosen for all $\bar{\mu}_m$
because of a discrete situation of previous conditioning optimal plan calculation.
See the now ``trivial'' \cref{formula_barycenter},
we can use it to conclude uniform Lipschitz constant for $m$.

Recall for open set $E \subset M$, $\bar{\mu}(E) \leq \liminf \bar{\mu}_m(E)$,
as indicator function of open set is lower semi-continuous.
Hence \cref{equa:absolutely_continous} holds for all open sets.
Es Borel measure is outer regular, for general measurable $E$ with $\text{Vol}(E) < \frac{\delta}{2}$
select an open set $E^\prime$ such that
$ E \subset E^\prime$ and $ \text{Vol}(E^\prime) < \delta$,
then $\bar{\mu}(E) \leq \bar{\mu}(E^\prime) < \epsilon$.

Previous argument works for general $\lambda_1 \delta_{\mu_1} + \lambda_2 \mathbb{P}$ for any $\mathbb{P} \in P(P(M))$
if we approxiamte $\mathbb{P}$ by finite supported measures.
To sum up, we should be able to single out a part of $\mathbb{P}$ in the form $\lambda_1 \delta_{\mu_1}$
with lower bound on $\lambda_1$ and dominated $\mu_1$.
One way to do so is to assume $\mu_1$ has bounded density.
\begin{defn}
	[The set $ \mathcal { A } _ { L }$]
	For \( 0 < L < \infty \), let \( \mathcal { A } _ { L } \) be the set of Borel probability
	measures on \( M \), absolutely continuous with respect to volume, whose densities have \( L ^ { \infty } \)
	norm less than or equal to \( L\).
\end{defn}

Note that, since the bound on the \( L ^ { \infty } \) norm is preserved under weak-* convergence,
\( \mathcal { A } _ { L } \) is a weakly-* closed, and thus Borel measurable, subset of \( P ( M ) \).

If $\mu_1 \in \mathcal{A}_L$, then by duality between $L^p(\bar{\mu})$ and $L^q(\bar{\mu})$
and that they are separable spaces,
we have that $\bar{\mu}$ has density in $L^p(\bar{\mu})$ for
$p < \infty$ with a upper norm bound depending only on constat $L$ (, $\lambda_1$ and $M$).
We pass $p$ to infity to get $\bar{\mu} \in \mathcal{A}_L$.
Hence, our conclusion holds for any $\mathbb{P}$ that is not atomless on $\mathcal{A}_L$.

Finally, if we are given $\mathbb{P}$ with only assumption that $\mathbb{P}(\mathcal{A}_L(M)) > 0$,
we need more control on the density function of $\bar{\mu}_m$
to ``remove'' the dependency of its upper bound on a singled-out coefficient $\lambda_1$.
One hope is that $\lambda_1$ is replaced by $\mathbb{P}(\mathcal{A}_L(M))$.

\subsection{Calculate density function}

Every measure $\mu$ on $M$ in following discussion is absolutely continous.
We use the convention that denote by $g$ the density function for absolutely measure $\mu$.
One principle of differential geometry is to differentiate everything we could.

% If we only apply general bound on $c$-concave function
Recall change of variable (Theorem 4.2 in \cite{cordero2001riemannian}) and notations in \cref{prop:differentiate_optimal_transport},
\[
	\bar{g} = g_i \circ T_i \det D T_i :
	= g_i \circ T_i \det[Y(H-\text{Hess} u_i]
\]
where $T_i = \exp(-\nabla u_i)$ is the unique optimal maps from $\bar{\mu}$ to $\mu_i$.

For general $c$-concave function $\mu_i$, we have only upper hessian bound.
And this is not enough to get an estimation on the absolute value of Jacobian determinants.
% For instance, in Euclidean space, $DT_i = (\lambda_i -1)/\lambda_i \leq \text{Id}$.

We differentiate the equality $B(T_1(x), \ldots, T_n(x))=x$ for $\bar{\mu}$-a.e. $x$,
\begin{align*}
	\text{Id} =\sum_{i=1}^n \partial_i B\, DT_i
&=\sum_{i=1}^n D \exp(-\frac{1}{\lambda_i}\nabla f_i) \, D \exp(-\nabla u_i)\\
&=\sum_{i=1}^n D \exp^{-1}(-\nabla \left( \frac{f_i}{\lambda_i}\right)^c) \, D \exp(-\nabla u_i)\\
&=\sum_{i=1}^n(H-\text{Hess}(f_i / \lambda_i)^c)^{-1}\,Y_i^{-1}\,
Y_i\,(H-\text{Hess} u_i)\\
&=\sum_{i=1}^n(H-\text{Hess}(f_i / \lambda_i)^c)^{-1}\,
(H-\text{Hess}u_i) .
\end{align*}

Then by Minkowski's determinant inequality, we get
\begin{align*}
	1 &\geq \sum_{i=1}^{n} \det [H-\text{Hess}(f_i/\lambda_i)^c]^{-1/n}\,\det[H-\text{Hess}u_i]^{1/n}\\
		&=\sum_{i=1}^n \det[\partial_i B]^{1/n}\,\det[DT_i]^{1/n}
\end{align*}
Recall that we know $(f_i / \lambda_i)^c$ has hessian bound from both sides, see \cref{equa:hessian_bound_f}.
From it we get $\det[\partial_i B]^{1/n} \geq \min \{1, \lambda_i / (1 - \lambda_i)\} > \lambda_i$,
where $C > 0$ depends only on hessian bound of square distance function and Lipschitz constant of exponential map.

Combine these two inequalities, and we then apply Jensen inequlity
\[
	\bar{g} \leq 
	\left[ \sum_{i=1}^n \frac{\det[\partial_i B]^{1/n}}
	{g_i^{1/n} \circ T_i}\right]^{-n}
	< \left[ \sum_{i=1}^n \frac{C \, \lambda_i}
	{g_i^{1/n} \circ T_i}\right]^{-n}
	\leq C^{-n} \sum_{i=1}^n \lambda_i g_i \circ T_i.
\]
With this estimation in hand,
one shows easily that if measure $\mathbb{P} \in P(P(M))$ on $P(M)$ give mass to closed set $\mathcal{A}_L$,
then it has a unique absolutely continous barycenter.

\subsubsection{Jacobian determinant inequality for the Wasserstein barycenter}

This is done by Kim and Pass.
\begin{defn}[Volume distortion]
	Let \( \lambda \) be a Borel probability measure on \( M \) with a
	unique barycenter \( \bar { x } \) (that is, such that \( B C ( \lambda ) \) is a singleton). We define the generalized,
	or barycentric, volume distortion coefficients at \( y \notin \operatorname { cut } ( \bar { x } ) \)

	\[ \alpha _ { \lambda } ( y ) : = \frac { \operatorname { det } \left[ - \left. D _ { y z } ^ { 2 } \right| _ { z = \bar { x } } c ( y , z ) \right] } { \operatorname { det } \left[ \left. \int _ { M } D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } c ( x , z ) d \lambda ( x ) \right] } \]
	where \( D _ { z z } ^ { 2 } c ( x , z ) \) denotes the Hessian of the function \( z \mapsto c ( x , z ) \), and the determinants
	are computed in exponential local coordinates at \( \bar { x } \) and \( y . \)
\end{defn}

\begin{thm}
	[Jacobian determinant inequality for the Wasserstein barycenter]
	Assume that the Wasserstein barycenter \( \bar { \mu } \) of the measure \( \Omega \) on \( P ( M ) \) is absolutely continuous.
	Letting \( T _ { \mu } \) denote the optimal map from \( \bar { \mu } \) to \( \mu \), consider the measure on \( M \) given by
	\[ \lambda _ { x } : = \int _ { P ( M ) } \delta _ { T _ { \mu } ( x ) } d \Omega ( \mu ) \]
	which is defined with respect to a.e. $x$.
	Then, for \( \bar { \mu } \)-a.e. \(x\),
	\[ 1 \geq \int _ { P ( M ) } \alpha _ { \lambda _ { x } } ^ { 1 / n } \left( T _ { \mu } ( x ) \right) \operatorname { det } ^ { 1 / n } D T _ { \mu } ( x ) d \Omega ( \mu ) \]
\end{thm}

% \subsubsection{Use local coordinate}
% Need to work on it.

\subsubsection{Use Skorohod representation}
\textcolor{red}{This is not possible!}
One may consider to construct absolutely continous random variables.
For example, to use the Skorohod representation (see section 8.5 in \cite{Bogachev2007}),
\begin{defn}
	We shall say that a topological space \( X \) has the strong
	Skorohod property for Radon measures if to every Radon probability measure
	\( \mu \) on \( X \),
	one can associate a Borel mapping \( \xi _ { \mu } : [ 0,1 ] \rightarrow X \) such that \( \mu \) is
	the image of Lebesgue measure under the mapping \( \xi _ { \mu } \) and \( \xi _ { \mu _ { n } } ( t ) \rightarrow \xi _ { \mu } ( t ) \) a.e.
	whenever the measures \( \mu _ { n } \) converge weakly to \( \mu . \)
\end{defn}

However, even if $\mu$ is absolutely continous respect to Lebesgue measure on $\mathbb{R}$,
we don't have necessarily that $\xi_{\mu}$ is a absolutely continous function.
In fact, the Housdorff dimension of the image of $\xi_{\mu}$ is not likely to be greater than 2.
See \cite{Besicov1937Sets} for discussions on $\delta$-Lipschitz curves,
their Housdorff dimensions are bounded by $2-\delta$.

% \subsection{Control on density function}
