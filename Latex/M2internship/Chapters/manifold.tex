%! TEX root = ../barycenter.tex
\chapter{Wasserstein space over Riemannian manifold}
\section{Convex analysis on Riemannian manifold}
\subsection{Polar factorization of maps on manifolds}

\begin{lem} [Lipschitz cost]
	\label{lem:Lipschitz_cost}
	Let \( ( M , d ) \) be a metric space whose diameter \( | M | : = \sup \{ d ( x , z ) \mid x , z \in M \} \) is finite.
	For each \( y \in M \), the function \( \psi ( x ) = d ^ { 2 } ( x , y ) / 2 \) is Lipschitz continuous:
	\begin{equation}
		\label{equa:Lipschitz}
		| \psi ( x ) - \psi ( z ) | \leq | M | d ( x , z ).
	\end{equation}
\end{lem}

\begin{proof}
	The triangle inequality shows that \( \phi ( x ) : = d ( x , y ) \) has Lipschitz constant one:
	\[ \phi ( x ) - \phi ( z ) = d ( x , y ) - d ( z , y ) \leq d ( x , z ) \]
	for all \( x , z \in M\).
	Also, \( \phi ( x ) = d ( x , y ) \leq | M | < \infty \) is bounded.
	The desired estimate \cref{equa:Lipschitz} then follows easily for \( \psi ( x ) = \phi ^ { 2 } ( x ) / 2 \) :
	\begin{align*}
		2 | \psi ( x ) - \psi ( z ) | & = | \phi ( x ) ( \phi ( x ) - \phi ( z ) ) + \phi ( z ) ( \phi ( x ) - \phi ( z ) ) | \\
		                              & \leq | M | d ( x , z ) + | M | d ( x , z )
	\end{align*}
\end{proof}

\begin{lem} [Infimal convolutions are Lipschitz]
	\label{lem:infimal_convolution_Lipschitz}
	Fix a metric space \( ( M , d ) \) having finite diameter.
	Any \( \psi : M \rightarrow \mathbf { R } \cup \{ \pm \infty \} \) given by an infimal convolution \( \psi = \psi ^ { c c } \) with \( c ( x , y ) = d ^ { 2 } ( x , y ) / 2 \) is either identically infinite \( \psi = \pm \infty \) or Lipschitz continuous throughout \( M\).
	Indeed, it satisfies \cref{equa:Lipschitz}.
\end{lem}

\begin{proof}
	More generally, suppose \( \psi = \phi ^ { c } \) for some \( \phi : M \rightarrow \mathbf { R } \cup \{ \pm \infty \} \) meaning
	\begin{equation}
		\label{equa:infimal_convolution}
		\psi ( x ) = \inf _ { y \in M } c ( x , y ) - \phi ( y )
	\end{equation}
	Observe \( 0 \leq c ( x , y ) \leq | M | ^ { 2 } / 2 \) is bounded.
	Either \( \phi \) is unbounded above, in which case \( ( 11 ) \) yields \( \psi = - \infty \) and the lemma holds trivially, or else \( \psi \) is bounded below.
	Fix \( z \in M \), and note \( \psi ( z ) = + \infty \) in \cref{equa:infimal_convolution} occurs only if \( \phi : = - \infty \) everywhere, in which case \( \psi = + \infty \) again holds trivially.
	Thus we may assume that \( \psi \) is finite everywhere.
	Given any \( \epsilon > 0 \), there exists \( y \in M \)
	such that \( \psi ( z ) + \epsilon \geq c ( z , y ) - \phi ( y ) \), while \( \psi ( x ) \leq c ( x , y ) - \phi ( y ) \) holds because of \cref{equa:infimal_convolution}.
	Subtracting these two inequalities yields
	\begin{align*}
		\psi ( x ) - \psi ( z ) & \leq c ( x , y ) - c ( z , y ) + \epsilon \\
		                        & \leq | M | d ( x , z ) + \epsilon
	\end{align*}
	by \cref{lem:Lipschitz_cost}. Since the last inequality holds for all \( \epsilon > 0 \), the Lipschitz
	estimate \cref{equa:Lipschitz} has been proved.
\end{proof}

One reason to study square distance on manifold could be that under Taylor expansion, it is directlt related to inner product of Riemannian metric.
An example is to prove superdifferentiability of geodesic distance squared in \cite{mccann2001polar}.

\begin{prop}
	[Superdifferentiability of geodesic distance squared]
	Let \( ( M , g ) \) be a \( C ^ { 3 } \)-smooth Riemannian manifold, possibly with boundary.
	Suppose \( \sigma : [ 0,1 ] \rightarrow M \) has minimal length among piecewise \( C ^ { 1 } \) curves joining \( y = \sigma ( 0 ) \) to \( x = \sigma ( 1 ) \notin \partial M \), parameterized with constant speed.
	Then \( \psi ( \cdot ) = d ^ { 2 } ( \cdot , y ) / 2 \) has supergradient \( \dot { \sigma } ( 1 ) \in \bar { \partial } \psi _ { x } \) at \( x \).
\end{prop}

\begin{proof}
	Since \( x \) lies in the interior of \( M \), there is some \( \epsilon > 0 \) and neighbourhood \( X \subset M \) of \( x \) such that: at each \( z \in X \), the exponential map exp \( _ { z } \) maps
	the ball \( \mathbf { B } ( \mathbf { 0 } , \epsilon ) \subset T M _ { z } \) diffeomorphically onto some open set \( U _ { z } \supset X \), as in Milnor.
	The proposition will first be established when
	\( y = \sigma ( 0 ) \in X \), in which case \( \psi \) is actually differentiable at \( \exp _ { y } \dot { \sigma } ( 0 ) = x \)
	We compute its derivative by linearizing exp \( _ { x } \mathbf { v } \in X \) around the origin and $\exp _ { y }$ around $ \dot { \sigma } ( 0 )$:
	\begin{align*}
		\psi \left( \exp _ { x } \mathbf { v } \right) & = d ^ { 2 } \left( y , \exp _ { y } \left( \exp _ { y } ^ { - 1 } \exp _ { x } \mathbf { v } \right) \right) / 2 \\ & = \left| \exp _ { y } ^ { - 1 } \left( \exp _ { x } \mathbf { v } \right) \right| _ { y } ^ { 2 } / 2 \\ & = \left| \dot { \sigma } ( 0 ) + D \left( \exp _ { y } ^ { - 1 } \right) _ { x } D \left( \exp _ { x } \right) _ { 0 } \mathbf { v } + o \left( | \mathbf { v } | _ { x } \right) \right| _ { y } ^ { 2 } / 2 \\ & = | \dot { \sigma } ( 0 ) | _ { y } ^ { 2 } / 2 + g \left\langle \dot { \sigma } ( 0 ) , \left( D \exp _ { y } \right) _ { \dot { \sigma } ( 0 ) } ^ { - 1 } I \mathbf { v } \right\rangle _ { y } + o \left( | \mathbf { v } | _ { x } \right) \\ & = d ^ { 2 } ( x , y ) / 2 + g \langle \dot { \sigma } ( 1 ) , \mathbf { v } \rangle _ { x } + o \left( | \mathbf { v } | _ { x } \right)
	\end{align*}
	so that \( \nabla \psi ( x ) = \dot { \sigma } ( 1 )\).

	Here the last equation follows from \( \dot { \sigma } ( 1 ) = \)
	\( D \left( \exp _ { y } \right) _ { \dot { \sigma } ( 0 ) } \dot { \sigma } ( 0 ) \) and Gauss' lemma.
\end{proof}

\subsection{A Riemannian interpolation inequality Ã  la Borell, Brascamp and Lieb}

We copy following results (tex code) from \cite{cordero2001riemannian}.

\begin{prop}
	[Distances fail to be semiconvex at the cut locus]
	\label{prop:distance_cut_locus}
	At each \( x \in \operatorname { cut } ( y ) \), the square distance \( \psi : = d _ { y } ^ { 2 } / 2 \) satisfies:
	\[ \inf _ { 0 < | v | < 1 } \frac { \psi \left( \exp _ { x } v \right) + \psi \left( \exp _ { x } - v \right) - 2 \psi ( x ) } { | v | ^ { 2 } } = - \infty \]
\end{prop}
\begin{defn} [$c$-transforms and the subset \( \mathcal { I } ^ { c } ( X , Y ) \) of \( c \)-concave functions]
	Let \( X \) and \( Y \) be two compact subsets of \( M \). The set \( \mathcal{I} ^ { c } ( X , Y ) \) of \( c \) -concave functions (relative to \( X \) and \( Y \) ) is the set of functions \( \phi \) : \( X \rightarrow \mathbf { R } \cup \{ - \infty \} \) not identically \( - \infty \), for which there exists a function \( \psi : Y \rightarrow \mathbf { R } \cup \{ - \infty \} \) such that
	\begin{equation}
		\label{defn:c_transform}
		\phi ( x ) = \inf _ { y \in Y } c ( x , y ) - \psi ( y ) \quad \forall x \in X.
	\end{equation}
	We refer to \( \phi \) as the \( c \)-transform of \( \psi \) and abbreviate \cref{defn:c_transform} by writing \( \phi = \psi ^ { c } \)
	Similarly, given \( \phi \in I ^ { c } ( X , Y ) \), we define its \( c \) -transform \( \phi ^ { c } \in \mathcal{I} ^ { c } ( Y , X ) \) by
	\[ \phi ^ { c } ( y ) : = \inf _ { x \in X } c ( x , y ) - \phi ( x ) \quad \forall y \in Y. \]
\end{defn}

\begin{thm}[Optimal mass transport on manifolds]
	\label{thm:optimal_transport_manifold}
	Let \( M \) be a complete, continuously curved Riemannian manifold. Fix two Borel probability measures \( \mu \ll \) vol and \( v \) on \( M \), and two compact subsets \( X \) and \( Y \subset M \) containing the support of \( \mu \) and \( v \), respectively. Then there exists \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \) such that the map
	\begin{equation}
		\label{equa:transform_map}
		F ( x ) : = \exp _ { x } ( - \nabla \phi ( x ) )
	\end{equation}
	pushes \( \mu \) forward to \( v \). This map is uniquely characterized among all maps pushing \( \mu \) forward \( v \) by formula \cref{equa:transform_map} with \( \phi \in \mathcal{I} ^ { c } ( X , Y ) . \) Furthermore \( F \) is the unique minimizer of the quadratic cost \( \int d ^ { 2 } ( x , G ( x ) ) d \mu ( x ) \) among all Borel maps \( G : M \rightarrow M \) pushing \( \mu \) forward to \( v \) (apart from variations on sets of \( \mu \)-measure zero).
\end{thm}

The map \( F \) may be referred to either as the optimal map or optimal mass
transport between \( \mu \) and \( v \).

Let us also recall one of the basic lemmas from its proof, which illumi-
nates the structure of the map \( F \). Given two compact subsets \( X \) and \( Y \subset M \)
with \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \), one sees every \( ( x , y ) \in X \times Y \) satisfy
\begin{equation}
	\label{equa:c-concave_conjugate}
	c ( x , y ) - \phi ( x ) - \phi ^ { c } ( y ) \geq 0
\end{equation}
with equality when \( \phi ( x ) = \inf _ { y ^ { \prime } \in Y } c \left( x , y ^ { \prime } \right) - \phi ^ { c } \left( y ^ { \prime } \right) = c ( x , y ) - \phi ^ { c } ( y ) . \)

\begin{lem}
	[Elementary properties of \( c \)-concave functions]
	\label{lem:minimizer_differentiable}
	Fix \( x \subset \subset M \) open and \( Y \subset M \) compact. For \( \phi \in I ^ { c } ( \bar { X } , Y ) \) define \( F ( x ) : = \)
	\( \exp _ { x } ( - \nabla \phi ( x ) ) \).
	\begin{enumerate}
		\item The function \( \phi \) is Lipschitz on \( \bar { X } \) and hence differentiable almost every-
		      where on \( X \).
		\item Fix any point \( x \in X \) where \( \phi \) is differentiable. Then \( y = F ( x ) \) if and
		      only if \( y \) minimizes \cref{equa:c-concave_conjugate} among \( y ^ { \prime } \in Y . \) In the latter case one has
		      \( \nabla \phi ( x ) = \nabla d _ { y } ^ { 2 } ( x ) / 2 \).
	\end{enumerate}
\end{lem}

\begin{defn}
	[$c$-superdifferential \( \partial ^ { c } \phi \)]
	Let \( X , Y \) be two compact sets of M. For \( \phi \in \mathcal { I } ^ { c } ( X , Y ) \)
	and \( x \in X \), the \( c \)-superdifferential of \( \phi \) at \( x \) is the non-empty set
	\begin{align}
		\partial ^ { c } \phi ( x ) & : = \left\{ y \in Y \mid \phi ( x ) + \phi ^ { c } ( y ) = c ( x , y ) \right\}                   \\
		                            & = \{ y \in Y \mid \phi ( z ) \leq \phi ( x ) + c ( z , y ) - c ( x , y ) \quad \forall z \in X \}
		\label{equa:c-superdifferential}
	\end{align}
\end{defn}

\begin{example} [Multivalued extension]
	\label{example:minimizer_differentiable}
	If \( \phi \in \mathcal { I } ^ { c } ( \bar { X } , Y ) \) is differentiable at
	\( x \in \mathcal { X } \subset \subset \),
	then \( \partial ^ { c } \phi ( x ) = \{ F ( x ) \} = \left\{ \exp _ { x } ( - \nabla \phi ( x ) ) \right\} \) according to \cref{lem:minimizer_differentiable}.
\end{example}

First recall that a geodesic ball \( B _ { r } ( x ) \) of radius \( r \) around \( x \in M \) is said to be embedded if the exponential map \( \exp _ { x } : \tilde { B } _ { r } ^ { x } ( 0 ) \rightarrow B _ { r } ( x ) \) defines a diffeomorphism from the open ball \( \tilde { B } _ { r } ^ { x } ( 0 ) \subset T _ { x } M \) onto \( B _ { r } ( x ) \subset M \).
A geodesic ball \( B _ { r } ( x ) \) around \( x \) is a convex embedded ball if it is embedded and geodesically convex---meaning every pair of points \( y , z \in B _ { r } ( x ) \) are joined by a unique geodesic of length less than \( 2 r \), and this geodesic is contained in \( B _ { r } ( x )\).
Small enough balls are always convex embedded balls.

\begin{defn}[Semi-concavity]
	\label{defn:semi-concavity}
	Fix \( \Omega \subset M \) open. A function \( \phi : \Omega \rightarrow \mathbf { R } \) is semi-concave at \( x _ { 0 } \in \Omega \) if there exists a convex embedded ball \( B _ { r } \left( x _ { 0 } \right) \) and a smooth function \( V : B _ { r } \left( x _ { 0 } \right) \rightarrow \mathbf { R } \) such that \( \phi + V \) is geodesically concave throughout \( B _ { r } \left( x _ { 0 } \right) . \) The function \( \phi \) is semi-concave on \( \Omega \) if it is semi-concave at each point of \( \Omega \).
\end{defn}

From the proof of Lemma 3.11 in \cite{cordero2001riemannian}, one can actually choose the local smooth function in \cref{defn:semi-concavity} as square distance function.

\begin{defn}[Hessian]
	Let \( \phi : \Omega \rightarrow \mathbf { R } \) be semi-concave on an open set \( \Omega \subset M . \) We say that \( \phi \) has \( a \) Hessian \( H \) at \( x \in \Omega \) if \( \phi \) is differentiable at \( x \)
	and there exists a self-adjoint operator \( H : T _ { x } M \rightarrow T _ { x } M \) satisfying
	\begin{equation}
		\label{defn:hessian}
		\sup _ { v \in \partial \phi \left( \exp _ { x } u \right) } \left| \Pi _ { x , u } v - \nabla \phi ( x ) - H u \right| = o ( | u | )
	\end{equation}
	as \( u \rightarrow 0 \) in \( T _ { x } \) M. Here \( \Pi _ { x , u } : T _ { \exp _ { x } u } M \rightarrow T _ { x } M \) denotes parallel translation to \( x \) along \( \gamma ( t ) : = \exp _ { x } ( t u ) . \) The Hessian of \( \phi \) at \( x \) may also be denoted
	by \( \operatorname { Hess } _ { x } \phi : = H \).
\end{defn}

This definition coincides with the usual one for smooth functions. A more intuitive understanding of the Hessian follows from the fact that existence of a Hessian \( H \) at \( x \) for \( \phi \) implies a second order Taylor expansion for \( \phi \) around \( x : \) as \( u \rightarrow 0 \in T _ { x } M \),
\begin{equation}
	\label{equa:hessian_expan}
	\phi \left( \exp _ { x } u \right) = \phi ( x ) + \langle \nabla \phi ( x ) , u \rangle + \frac { 1 } { 2 } \langle H u , u \rangle + o \left( | u | ^ { 2 } \right)
\end{equation}

It is remarkable that the converse also holds true: if \( \psi \) is semi-concave around \( x \) then \cref{defn:hessian} follows from \cref{equa:hessian_expan}.

\begin{prop}[\( c \)-concave functions are semi-concave]
	\( F i x X \subset \subset M \) open and \( Y \subset M \) compact. A \( c \)-concave function \( \phi \in \mathcal{I} ^ { c } ( \bar { X } , Y ) \) is semi-concave on \( X \) (and hence admits a Hessian \cref{defn:hessian} almost everywhere in \( X \)).
\end{prop}

\begin{thm}[Aleksandrov-Bangert, \cite{bangert1979analytiche} in German]
	Let \( \phi : \Omega \rightarrow M \) be semi-concave function on an open set \( \Omega \subset M . \) Then \( \phi \) admits a Hessian almost everywhere on \( \Omega \).
\end{thm}

This theorem is also proved as Theorem 14.1 in \cite{villani2008optimal}.
The notion of local semiconvexity
with quadratic modulus is invariant by $C^2$ diffeomorphism, so it suffices to prove for $\mathbb{R}^n$.


\begin{prop} [Differentiating optimal transport]
	\label{prop:differentiate_optimal_transport}
	Fix \( X \subset \subset M \) be openand \( Y \subset M \) compact. Let \( \phi \in I ^ { c } ( \bar { X } , Y ) \) and set \( F ( z ) : = \exp _ { z } ( - \nabla \phi ( z ) ) . \)
	Fix a point \( x \in X \) where \( \phi \) admits a Hessian \cref{defn:hessian}.
	Then:
	\begin{enumerate}
		\item \( y : = F ( x ) \notin \operatorname { cut } ( x ) \) and setting \( H : = \operatorname { Hess } _ { x } d _ { y } ^ { 2 } / 2 \), one has \( H - \operatorname { Hess } _ { x } \phi \) \( \geq 0 \)
		\item Introduce \( Y : = d \left( \exp _ { x } \right) _ { - \nabla \phi ( x ) } \) and define \( d F _ { x } : T _ { x } M \longrightarrow T _ { y } M \) by
		      \( d F _ { x } : = Y \left( H - \operatorname { Hess } _ { x } \phi \right) \). Then as \( u \rightarrow 0 \) in \( T _ { x } M \),
		      \begin{equation}
			      \sup _ {\substack {\exp _ { y } v \in \partial ^ { c } \phi \left( \exp _ { x } u \right) \\ | v | = d \left( y , \exp _ { y } v \right)} } \left| v - d F _ { x } ( u ) \right| = o ( | u | )
		      \end{equation}

	\end{enumerate}
\end{prop}

We only copy proof for the first conclusion, see Proposition 4.1 in \cite{cordero2001riemannian} for full detail.
\begin{proof}
	Suppose \( \phi \) admits a Hessian \cref{defn:hessian} at \( x \in X \).
	Then \( \phi \) is differentiable at \( x \) and
	\cref{example:minimizer_differentiable} shows that \( \partial ^ { c } \phi ( x ) = \{ F ( x ) \} = \{ y \} \).
	Thus for every \( z \in \mathcal { X } \), \cref{equa:c-superdifferential} yields
	\begin{equation}
		\label{equa:c-concave_distance_compare}
		\phi ( z ) \leq \phi ( x ) + d _ { y } ^ { 2 } ( z ) / 2 - d _ { y } ^ { 2 } ( x ) / 2 \end{equation}
	Taking \( z = \exp _ { x } ( \pm u ) \) and \( \psi : = d _ { y } ^ { 2 } / 2 \) gives
	\[ \frac { \phi \left( \exp _ { x } u \right) + \phi \left( \exp _ { x } - u \right) - 2 \phi ( x ) } { | u | ^ { 2 } } \leq \frac { \psi \left( \exp _ { x } u \right) + \psi \left( \exp _ { x } - u \right) - 2 \psi ( x ) } { | u | ^ { 2 } } \]
	As \( | u | \rightarrow 0 \) the left hand side tends to \( \left\langle \operatorname { Hess } _ { x } \phi ( u ) , u \right\rangle \) by hypothesis, so the right hand side is bounded below.
	Then \cref{prop:distance_cut_locus} ensures that
	\( x \notin \operatorname { cut } ( y ) \), or equivalently \( y \notin c u t ( x ) \).
	From \cref{equa:c-concave_distance_compare} we also observe that the function
	\[ h ( z ) : = d _ { y } ^ { 2 } ( z ) / 2 - \phi ( z ) \]
	has a minimum at \( z = x \). The Taylor expansion \cref{equa:hessian_expan} then implies the
	existence and non-negativity of its Hessian: Hess\(_{ x } h = H - \) Hess\(_ { x } \phi \geq 0\).
\end{proof}

\begin{thm}
	[Jacobian identity a.e.]
	\label{thm:jacobian_identity}
	Let \( \mu \ll \) Vol and \( \nu \ll \) Vol be
	two compactly supported Borel probability measures and denote their
	\( L ^ { 1 } ( M, \text{Vol}) \) densities by \( f \) and \( g \), respectively.
	Fix domains \( \mathcal{X} \subset \subset M \) and \( \mathcal{Y} \subset \subset M \) containing the support of \( \mu \) and \( v \), respectively.
	Suppose \( \phi \in \mathcal{I} ^ { c } ( \bar { \mathcal{X} } , \bar { \mathcal{Y} } ) \) induces \( F : \mathcal{X} \longrightarrow \bar { \mathcal{Y} } \) defined by \( F ( x ) : = \exp _ { x } ( - \nabla \phi ( x ) ) \)
	which pushes \( \mu \) forward to $\nu$.
	Then there exists a Borel set \( K \subset \mathcal{X} \) of full
	measure for \( \mu \) such that
	\begin{enumerate}
		\item $\phi$ admits a Hessian \( \operatorname { Hess } _ { x } \phi \) at each \( x \in K \), and hence \( F ( x ) \notin \operatorname { cut } ( x ) \).
		\item For \( x \in K \), setting \( Y : = d \left( \exp _ { x } \right) _ { - \nabla \phi ( x ) } \) and \( H : = \operatorname { Hess } _ { x } d _ { F ( x ) } ^ { 2 } / 2 \), one
		      has
		      \[ f ( x ) = g ( F ( x ) ) \operatorname { det } \left[ Y \left( H - \operatorname { Hess } _ { x } \phi \right) \right] \neq 0 \]
	\end{enumerate}
\end{thm}

\section{Paper review}

This is review of \cite{KIM2017640},
Wasserstein barycenters over Riemannian manifolds.

\subsection{Remind of basics in Riemannian geometry}

For \( c ( x , y ) = \frac{ 1 } { 2 } d ^ { 2 } ( x , y ) \), to show \( - D _ { x } c ( x , y ) = \exp _ { x } ^ { - 1 } ( y ) \), we should use exponetial coordinate at $T_yM$.
By Gauss lemma, we have \( \nabla r = \partial _ { r } \).
As $d c = r dr$, $\nabla c = r \partial r$ and our conculsion follows from that $ r \partial r $ is of length $r$.

It is instructive to recall proof of Gauss lemma in \cite{Petersen2016}.
On \( U = \exp _ { p } ( B ( 0 , \varepsilon ) ) \) we define the function \( r ( x ) = \left| \exp _ { p } ^ { - 1 } ( x ) \right| . \)
That is, \( r \) is simply the Euclidean distance function from the origin on \( B ( 0 , \varepsilon ) \subset T _ { p } M \) in exponential coordinates.
This function can be continuously extended to \( U \) by defining \( r ( \partial U ) = \varepsilon . \)

We know that \( \nabla r = \partial _ { r } = \frac { 1 } { r } x ^ { i } \partial _ { i } \) in Cartesian coordinates
on \( T _ { p } M . \)
We show that this is also the gradient with respect to the general metric \( g . \)

\begin{lem}
	[The Gauss Lemma]
	On \( ( U , g ) \) the function \( r \) has gradient \( \nabla r = \partial _ { r } \), where \( \partial _ { r } = D \exp _ { p } \left( \partial _ { r } \right) \).
\end{lem}

\begin{proof}
	We select an orthonormal basis for \( T _ { p } M \) and introduce
	Cartesian coordinates.
	These coordinates are then also used on \( U \) via the exponential map.
	Denote these coordinates by \( \left( x ^ { 1 } , \ldots , x ^ { n } \right) \) and the coordinate vector fields by
	\( \partial _ { 1 } , \ldots , \partial _ { n } . \)
	Then
	\begin{align*}
		r ^ { 2 }        & = \left( x ^ { 1 } \right) ^ { 2 } + \cdots + \left( x ^ { n } \right) ^ { 2 } , \\
		\partial _ { r } & = \frac { 1 } { r } x ^ { i } \partial _ { i }.
	\end{align*}
	For this, take a function $f: M \mapsto \mathbb{R}$, we have $ \frac{\partial f}{\partial x_i}=\frac{\partial f}{\partial r}\cdot \frac{\partial r}{\partial x_i}$.
	Differentiate $ r ^ { 2 } = \left( x ^ { 1 } \right) ^ { 2 } + \cdots + \left( x ^ { n } \right) ^ { 2 } $, we get $\frac{\partial r}{\partial x_i} = \frac{x_i}{r} $.
	Apply this equality agian, we can solve $\frac{\partial f}{\partial r}$ from $\frac{\partial f}{\partial x_i}$.

	To show that this is the gradient field for \( r ( x ) \) on \( ( M , g ) \), we must prove that \( d r ( v ) = \)
	\( g \left( \partial _ { r } , v \right) . \)
	We already know that
	\[
		d r = \frac { 1 } { r } \left( x ^ { 1 } d x ^ { 1 } + \cdots + x ^ { n } d x ^ { n } \right),
	\]
	but have no knowledge of $g$, since it is just some abstract metric.

	One can show that \( d r ( v ) = g \left( \partial _ { r } , v \right) \) by using suitable Jacobi fields for \( r \) in place of \( v \). Let us start with \( v = \partial _ { r } . \)
	The right-hand side is 1 as the integral curves for \( \partial _ { r } \) are unit speed geodesics.
	The left-hand side can be computed directly and is also $1$.
	Next, take a rotational field \( J = - x ^ { i } \partial _ { j } + x ^ { j } \partial _ { i } , i , j = 1 , \ldots , n , i < j . \)
	In dimension $2$ this is simply the angular field \( \partial _ { \theta } \).
	An immediate calculation shows that the left-hand side vanishes: \( d r ( J ) = 0 \).
	For the right-hand side we first note that \( J \) really is a Jacobi field as \( L _ { \partial _ { r } } J = \left[ \partial _ { r } , J \right] = 0 . \)
	Using that \( \nabla _ { \partial _ { r } } \partial _ { r } = 0 \) we obtain
	\[ \begin{aligned}
			\partial _ { r } g \left( \partial _ { r } , J \right) & = g \left( \nabla _ { \partial _ { r } } \partial _ { r } , J \right) + g \left( \partial _ { r } , \nabla _ { \partial _ { r } } J \right) \\
			                                                       & = 0 + g \left( \partial _ { r } , \nabla _ { \partial _ { r } } J \right)                                                                   \\
			                                                       & = g \left( \partial _ { r } , \nabla _ { J } \partial _ { r } \right)                                                                       \\
			                                                       & = \frac { 1 } { 2 } D _ { J } g \left( \partial _ { r } , \partial _ { r } \right)                                                          \\
			                                                       & = 0
		\end{aligned} \]
	Thus \( g \left( \partial _ { r } , J \right) \) is constant along geodesics emanating from \( p \).
	To show that it vanishes first observe that
	\[ \begin{aligned} \left| g \left( \partial _ { r } , J \right) \right| & \leq \left| \partial _ { r } \right| | J |                                                                               \\
                                                                     & = | J |                                                                                                                  \\
                                                                     & \leq \left| x ^ { i } \right| \left| \partial _ { j } \right| + \left| x ^ { j } \right| \left| \partial _ { i } \right| \\
                                                                     & \leq r ( x ) \left( \left| \partial _ { i } \right| + \left| \partial _ { j } \right| \right)\end{aligned} \]
	Continuity of \( D \exp _ { p } \) shows that \( \partial _ { i } , \partial _ { j } \) are bounded near \( p .  \)
	Thus \( \left| g \left( \partial _ { r } , J \right) \right| \rightarrow 0 \) as \( r \rightarrow 0 .  \)
	This forces \( g \left( \partial _ { r } , J \right) = 0 .\)
	Finally, observe that any vector \( v \) is a linear combination of \( \partial _ { r } \) and rotational fields.
	This proves the claim.
\end{proof}

\subsection{Fix typos and correct statements}

We copy original statement and put reference number directly after it.

\begin{rmk}[Remark 2.2]
	Inspection of the proof above shows that \( ( M \), vol \( ) \) can be replaced with a \cancel{compact separable} metric space \( ( X , \nu ) \) equipped with a reference Borel measure \( \nu \).
\end{rmk}

We only need an outer regular reference measure. And Borel measure on metric space is regular, see Theorem 7.1.7 in \cite{Bogachev2007}.

\begin{prop}[Proposition 2.9 Distortion under Ric \( \geq 0 \)]
	Suppose the Ricci curvature of \( M \) is everywhere nonnegative, i.e., Ric \( \geq 0 . \) Then, for any \( x \in M \) and \( \lambda \in P ( M ) \), \textcolor{blue}{if $\lambda$ gives no mass to the cut-locus of its baryceter $\bar{x}$}, we have
	\[ \alpha _ { \lambda } ( x ) \geq 1 \]
\end{prop}

\begin{proof}[Proof of Prop 2.9]
	Minimality of \( z \mapsto \int _ { M } c ( x , z ) \diff \lambda ( x ) \) at the barycenter \( \bar { x } \), combined with semi-concavity of \( z \mapsto c ( x , z ) \) and Fatou's lemma yields
	\[
		\int _ { M } \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } c ( x , z ) \diff \lambda ( x ) \geq 0
	\] as a matrix.
\end{proof}

Locally $c(x,z)=:d^2_x(z)$ near $\bar{x}$ is a geodesically convex function, by linear intergration so is the integral  \( z \mapsto \int _ { M } d^2_x(z) \diff \lambda ( x ) \).
As convexity implies local Lipschitz, $d^2_x(z)$ is also differentiable near $\bar{x}$ and so is its integral by compactness of $M$.
Thus we have locally for $\lambda$-almost all $x$ (not in the cut-locus of $\bar{x}$, arguing with semi-concavity is not strong as this),
% \textcolor{red}{why differentiability of $d^2_x(z)$ implies smoothness?}
\begin{equation}
	\label{equa:convex_distance_inequality}
	d^2_x \left( \exp _ {\bar{x}} u \right) - d^2_x (\bar{x}) - \langle \nabla d^2_x (\bar{x}) , u \rangle = \frac { 1 } { 2 } \langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z) \, u , u \rangle + o \left( | u | ^ { 2 } \right) > 0
\end{equation}

To show positivity of matrix, we take a vector $\nu$ and set $\mu = h \nu$ with $h > 0$ small enough.
Divide by $h$ in \cref{equa:convex_distance_inequality} and then take integral,
\[
	\langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } \int_{M} d^2_x(z) \diff \lambda ( x )\, v , v \rangle + \frac{o (h^2)}{h^2} = \int_{M} \langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z)\, v , v \rangle + \frac{o (h^2)}{h^2} \diff \lambda ( x ).
\]
Note that here we need compactness of $M$, hence boundness of $ \nabla d^2_x(\bar{x}) $ for $x \in M$, to get inter-change of integral and gradient:
\[
	\int_M \langle \nabla d^2_x (\bar{x}) , u \rangle \diff \lambda (x) = \langle \int_M \nabla d^2_x (\bar{x}) \diff \lambda (x) , u \rangle
\]
Apply Fatou's lemma, let $f\downarrow 0$,
\[
	\langle \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } \int_{M} d^2_x(z) \diff \lambda ( x )\, v , v \rangle \leq \langle \int_{M} \left. D _ { z z } ^ { 2 } \right| _ { z = \bar { x } } d^2_x(z) \diff \lambda(x)\, v, v \rangle.
\]
And by minimallity of $\bar{x}$, we know left hand side is non-negative.

This argument is also used in Prop 4.2.

\begin{rmk}[Remark 3.2]
	... In fact, it holds for any (compact) metric
	space on which the optimal \cancel{maps} \textcolor{blue}{plans}, \( T _ { \# } \mu = \nu \), exist uniquely for any arbitrary absolutely continuous source measure \( \mu \).
\end{rmk}

\begin{lem}[Lemma  4.1  a.e. \( x \) and $ \Omega$-a.e. $\mu$ ]
	Let \( \bar { \mu } \in \cancel{P ( M )} \textcolor{blue}{P_{ac}(M)}\) and for each \( \mu \in P ( M ) \), let \( u _ { \mu } \) be the dual potential (whose gradient is uniquely determined \( \bar { \mu } \) almost everywhere) for the optimal transport problem between \( \bar { \mu } \) and \( \mu . \) Let \( \Omega \) be a Borel probability on \( P ( M ) . \) For volume almost all \( x , x \mapsto u _ { \mu } ( x ) \) is twice differentiable for \( \Omega \) -almost all \( \mu \in P ( M ) . \)
\end{lem}

\begin{proof}
	$\forall \mu, x \mapsto \mu_u(x)$ is twice differentiable for $\bar{\mu}$-a.e. $x$. Apply Fubini's theorem then.
\end{proof}

\begin{prop} [Prop 4.2 Derivatives inside the integral $\int _ { P ( M ) } \diff \Omega$]
	\begin{equation}
		\label{equa:first_order}
		\nabla _ { x } \int _ { P ( M ) } u _ { \mu } ( x ) \diff \Omega ( \mu ) = \int _ { P ( M ) } \nabla _ { x } u _ { \mu } ( x ) \diff \Omega ( \mu )
	\end{equation}
	\begin{equation}
		\label{equa:second_order}
		\nabla _ { x } ^ { 2 } \int _ { P ( M ) } u _ { \mu } ( x ) \diff \Omega ( \mu ) \geq \int _ { P ( M ) } \nabla _ { x } ^ { 2 } u _ { \mu } ( x ) \diff \Omega ( \mu )
	\end{equation}
\end{prop}

\begin{proof}
	This can be seen by applying the dominated convergence theorem for \cref{equa:first_order} due to uniform Lipschitzness \textcolor{blue}{by \cref{lem:infimal_convolution_Lipschitz}} of \( u _ { \mu } \) and Fatou's lemma for \cref{equa:second_order} due to the semi-convexity
	of \( u _ { \mu } . \)
\end{proof}

% This shouldn't be an independent proposition, rather we can only condsider it in the case of Theorem 4.4.
% $u_{\mu}$ is uniformly Lipschitz on $x\in M$, this shows that $\nabla_{x}u_{\mu}$ is finite but not necessarily bounded (dominated).
To get rid of measurable selection problem, we consider
\[ y \mapsto \int_{P(M)} c(y, T_{\mu}(x)) \diff \Omega ( \mu)\]
at point $x$.
Here we assume $\bar{\mu}$ is barycenter of $\Omega$ and $T_{\mu}$ is the transfer map from $\bar{\mu}$ to $\mu$.
Then this integral valued map is well-defined $\mu$-a.e. by Fubini's theorem.
By stability of Kantorobich potentials (Theorem 1.52 in \cite{Santambrogio2015}), $c(y, T_{\mu}(x))$ is continous with respect to $\mu \in P(M)$.
Fix a $\mu$, $c(y, T_{\mu}(x))$ is locally Lipschitz with respect to $y$.
By continuity Lipschitz inequality will hold as well in a neighbourhood of $\mu$.
Consider these local neighbourhoods cover on compact set $ M \times P(M)$,
we can then get a uniform boundness on
\[\left. D_y\right|_{y=x} c(y, T_{\mu}(x)) = - \nabla_x u_{\mu}(x).\]


\begin{lem} [Lem 4.3 Riemannian barycenter from Wasserstein barycenter]
	\label{lem:inverse_barycenter}
	Let \( \bar { \mu } \) be a Wasserstein barycenter of the measure \( \Omega \) on \( P ( M ) \) and assume \( \bar { \mu } \) is absolutely continuous with respect to volume;
	let \( T _ { \mu } \) be an optimal map from \( \bar { \mu } \) to \( \mu . \)
	Let \( \lambda _ { z } = \left( \mu \mapsto T _ { \mu } ( z ) \right) _ { \# } \Omega . \)
	Then, for \( \bar { \mu } \) almost every \( z , z \) is a barycenter of \( \lambda _ { z }\).
	\textcolor{blue}{And $\lambda_z$ gives no mass to the cut-locus of $z$} by \cref{prop:differentiate_optimal_transport}.

	If, in addition, \( \Omega \left( P _ { a c } ( M ) \right) > 0 \), then for \( \bar { \mu } \) almost every \( z , z \) is the unique barycenter of $\lambda _ { z }$.

\end{lem}

There is an explanation behind this lemma. Consider we have random images, and we want to approch a best representative of them by simulation. We process this by compose all barycenters of simulated images into an average image. On the other hand, for each grid point in our average image, we can simulate a new image by transfering that grid point optimally. This lemma claims that that choosen grid point should be a barycenter of the new generated image. This could be related to ergodic theory as there are two kinds of averages involved.


\begin{proof}[Proof of the 1st order balance]
	...
	% On the other hand, by Lemma  4.3, for \( \bar { \mu } \) almost every \( x \), we have that \( x \) is the
	% barycenter of \( \lambda _ { z } = \left( \mu \mapsto T _ { \mu } ( z ) \right) _ { \# } \Omega ; \) that is, a minimizer of
	% \[ f _ { x } : y \mapsto \int _ { P ( M ) } d ^ { 2 } \left( y , T _ { \mu } ( x ) \right) d \Omega ( \mu ) .\]

	Therefore, the latter function \( f _ { x } \), which is semi-concave is differentiable at \( x : \) due to semi-concavity, there is \( C > 0 \) such that the function \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) is locally geodesically concave near \( x \).
	Minimality at \( x \) implies \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \geq f _ { x } ( x ) - \) \( C \operatorname { dist } ^ { 2 } ( x , \cancel{x}\, \textcolor{blue}{y} ) \).
	Since \( y \mapsto f _ { x } ( x ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) has vanishing derivative at \( x \), concavity of \( f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) implies that locally the function \( y \mapsto f _ { x } ( y ) - C \operatorname { dist } ^ { 2 } ( x , y ) \) is also
	locally bounded from above by the constant \( f _ { x } ( x ) . \) This implies the differentiability of
	\( f _ { x } \) at \( x \).
\end{proof}

% \textcolor{cyan}{If we have} that $\lambda_{z}$ is absolutely continous, then we prove first order balance by taking gradient under integral.
% For the sencond paragraph of argument,
To simplify, we should prove:
\begin{lem}
	For $f, g$ two convex functions near $0$, assume $f(0)=g(0)=g^\prime(0)=0$. If $f \leq g$ and $0$ is a local minimum of $g$, we then have $f \geq 0$ and $f$ is differentiable at $0$.
\end{lem}

\begin{proof}
	We need to show that subdifferential $\partial f(0) $ of $f$ is a single point set $ \{ 0 \}$. $\forall u \in \partial f( 0 )$,
	\[
		\langle x, u \rangle \leq f(x) - f(0) = f(x) \leq g(x) = g(x) - g(0),
	\]
	that is say, $ u \in \partial g(0)$. Hence $ \partial f(0) = \partial g(0) = \{0\}$.

\end{proof}

\begin{proof}[Proof of Theorem 4.6]
	...\\
	Note that each \( - D _ { x y } ^ { 2 } c \left( x , T _ { \mu } ( x ) \right) D T _ { \mu } ( x ) = D _ { x x } ^ { 2 } u _ { \mu } ( x ) + D _ { x x } ^ { 2 } c \left( x , T _ { \mu } ( x ) \right) \) is positive semi-definite by the \( c \)-convexity of \( u _ { \mu } \), and hence so is their integral...
\end{proof}

By definition, we have
\[
	\inf_{\textcolor{cyan}{z}} u_\mu(\textcolor{green}{z}) + c(\textcolor{green}{z}, T_\mu(x))= u_\mu(x) + c(x, T_\mu(x)) = - u^c ( T_\mu(x)).
\]
If consider derivative with respect to first variable,
then we have first differential vanishes and Hessian semi-positive.


% \section{Multi-marginal optimal transport on Riemannian manifolds}
% We will prove uniqueness and Monge solution results for the multi-marginal problem on a compact Riemannian manifold,
% with cost function
% \begin{equation}
% 	\label{equa:mult-imarginal_problem}
% 	c \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) = \inf _ { y \in M } \sum _ { i = 1 } ^ { m } \frac { d ^ { 2 } } { 2 } \left( x _ { i } , y \right)
% \end{equation}

% \begin{lem}
% 	Fix \( \left( x _ { 1 } , \ldots , x _ { m } \right)\).
% 	Then any \( y \) which minimizes \( y \mapsto \sum _ { i = 1 } ^ { m } d ^ { 2 } \left( x _ { i } , y \right) \) is not in the cut locus of \( x _ { i } \) for any \( i . \)
% \end{lem}

% \begin{proof}
% 	Choose a point \( y \) in the cut locus of \( x _ { i } \) for some \( i \);
% 	we will show that \( y \) cannot minimize \( y \mapsto \sum _ { i = 1 } ^ { m } d ^ { 2 } \left( x _ { i } , y \right) . \)
% 	By Lemma 3.12 in \cite{cordero2001riemannian}, we can find a constant \( K \) such that, for all \( u \in T _ { y } M \), and \( j = 1,2 , \ldots m \), we have
% 	\[ \frac { d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) - 2 d ^ { 2 } \left( x _ { j } , y \right) } { | u | ^ { 2 } } \leq K \]
% 	On the other hand, by Proposition \( 2.5 \) in the same paper, we can find some non
% 	zero \( u \in T _ { y } M \) such that
% 	\[ \frac { d ^ { 2 } \left( x _ { i } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { i } , \exp _ { y } ( - u ) \right) - 2 d ^ { 2 } \left( x _ { i } , y \right) } { | u | ^ { 2 } } \leq - m K \]
% 	Therefore, we have
% 	\[ \begin{aligned} \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) & = \sum _ { j \neq i } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) + d ^ { 2 } \left( x _ { i } , y \right) \\ & \geq \frac { - ( m - 1 ) K | u | ^ { 2 } } { 2 } + \frac { 1 } { 2 } \sum _ { j \neq i } ^ { m } \left( d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) \right) \\ & + \frac { m K | u | ^ { 2 } } { 2 } + \frac { 1 } { 2 } d ^ { 2 } \left( x _ { i } , \exp _ { y } u \right) + d ^ { 2 } \left( x _ { i } , \exp _ { y } ( - u ) \right) \\ & > \frac { 1 } { 2 } \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) + \frac { 1 } { 2 } \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } ( - u ) \right) \end{aligned} \]
% 	Therefore, either
% 	\[ \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } u \right) < \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \]
% 	or
% 	\[ \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , \exp _ { y } - u \right) < \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \]
% 	in either case, \( y \) cannot minimize \( \sum _ { j = 1 } ^ { m } d ^ { 2 } \left( x _ { j } , y \right) \)
% \end{proof}

% \begin{lem}
% 	The cost function \( c \) is everywhere superdifferentiable with respect to \( x _ { 1 } \).
% 	That is, for all \( \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) \in M ^ { m } \) there exist \( p \in T _ { x _ { 1 } } M \) (the super-gradient) such that, for small \( v \in T _ { x _ { 1 } } M \), we have
% 	\[ c \left( \exp _ { x _ { 1 } } v , x _ { 2 } , \ldots , x _ { m } \right) \leq c \left( x _ { 1 } , x _ { 2 } , \ldots , x _ { m } \right) + g ( p , v ) + o ( | v | ) \]
% \end{lem}

% \begin{lem}
% 	At any point \( \left( x _ { 1 } , \ldots , x _ { m } \right) \) where \( c \) is differentiable with respect to
% 	\( x _ { 1 } \), there is a unique minimizing \( y \) in \cref{equa:mult-imarginal_problem}, and moreover,
% 	\[ y = \exp _ { x _ { 1 } } \left( \nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) \right) \]
% \end{lem}

% \begin{proof}
% 	For any minimizing \( y \) in \cref{equa:mult-imarginal_problem},
% 	\( d ^ { 2 } \left( x _ { 1 } , y \right) \) is differentiable as \( y \notin \operatorname { cut } \left( x _ { 1 } \right) \).
% 	We then have
% 	\[
% 		\nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) = \nabla _ { x _ { 1 } } \left( \frac { 1 } { 2 } d ^ { 2 } \left( x _ { 1 } , y \right) \right)
% 	\]
% 	This equation implies that \( y \) must equal \( \exp _ { x _ { 1 } } \left( \nabla _ { x _ { 1 } } c \left( x _ { 1 } , \ldots , x _ { m } \right) \right) \);
% 	uniqueness follows immediately.
% \end{proof}


\section{Barycenter on manifold}

% \subsubsection{A perspective from optimal transportation}

% Make use of previous discussions, we have following nice property

% \textcolor{red}{ALL THESE ARE WRONG! So I finally change orignal $i$ into $j$.
% That is to say, we fix $i$ in following lemma, everything is so trivial now.}

% \begin{lem}
% Let $\mu_j, j=1,2,\ldots,n$ be a $n$ measures on compact Riemanninan manifold $M$.
% Assume $\mu_i$ absolutely continuous, and other $\mu_j, j \neq i$ are discrete.
% % absolutely continous with respect to volume measure on $M$.
% Then the measure $\sum_{j=1}^n \lambda_j \delta_{\mu_j} \in P(P(M))$ has a unique barycenter $\bar{\mu}$ on $M$.
% $\bar{\mu}$ is absolutely continous with respect to volume measure on $M$.
For $n$ points $x_i \in M, i=1,2,\ldots,n$, we define a function on $M^n$,
\[
	f(x_1, x_2, \ldots, x_n) := \min_{w} \sum_{i=1}^n \lambda_i c(w, x_i) =: \frac{1}{2}W^2(\sum_{i=1}^n \lambda_i \delta_{x_i}, M).
\]
We define $f_i$ as function on $M$ through $f$ by fixing $x_j$ for $j \neq i$.
Consider function $g_i(w) := -\sum_{j\neq i}^n \lambda_j c(w, x_j) / \lambda_i$,
then by definition $f_i /\lambda_i$ is the $c$-conjugate of $g_i$.
Locally barycenters are contained in a compact set, a union of $n$ closed balls.
This implies that locally $f_i / \lambda_i $ is a $c$-concave function,
so it is semi-convex and it has hessian on $M$ almost everywhere with respect to the volume measure.

% with a global Lipschitz constant bounded by diameter of $M$.
% This conclusion relies heavily on compactness of manifold $M$.
% \end{lem}
The $c$-conjugation inequality
$ c(w, x_i) \leq g_i(w) + f_i / \lambda_i (x_i)$ 
becomes equality when $z$ is a barycenter of $\sum_{i=1}^{n} \lambda_i \delta_{x_i}$,
\begin{equation}
	\label{equa:g_i_conjugate}
	c(z, x_i) = g_i(z) + \frac{1}{\lambda_i} f_i(x_i).
\end{equation}
If in addition, $f_i / \lambda_i$ has hessian at $x_i$,
then \cref{equa:g_i_conjugate} is differentiable at $x_i$,
$z$ is not in the cut-locus of $x_i$;
$x_i$ is then not in the cut-locus of $z$,
so \cref{equa:g_i_conjugate} is also differentiable at $z$.
Differentiate \cref{equa:g_i_conjugate} with respect to $x_i$ and $z$ and use the minimallity property,
we get
\begin{align*}
	z &= \exp_{x_i}( - \frac{1}{\lambda_i} \nabla f_i),\\
	x_i &= \exp_{z}( - \nabla g_i).
\end{align*}

We have in general that the $c$-conjugate of $f_i / \lambda_i$ satisfies
$(f_i / \lambda_i)^c = g_i^{cc} \geq g_i$,
and thus $c(w, x_i) \leq g_i(w) + f_i/\lambda_i (x_i) \leq (f_i / \lambda_i)^c(w) + f_i/\lambda_i (x_i)$.
This long inequality implies that any $w$ in the equality
$c(x_i, w) = f_i / \lambda_i (x_i) + (f_i / \lambda_i)^c(w)$ must be a barycenter of $\sum_{i=1}^{n} \lambda_i \delta_{x_i}$.
Moreover, we have following equality holds whenever one of gradients exits,
\[\exp(-\nabla g_i) = \exp(-\nabla (f_i / \lambda_i)^c) = \exp^{-1}(-\nabla f_i / \lambda_i).\]
Note cut-locus are excluded from consideration in above equality,
and as a corollary $\nabla g_i = \nabla (f_i / \lambda_i)^c$.
Therefore, two locally Lipschitz function $g_i$ and $f_i /\lambda_i$ coincide almost everywhere.
If follows $g_i=f_i /\lambda_i$ on $M$.

Denote by $B$ a measurable selection of barycenter of
$n$-points supported measure $\sum_{i=1}^{n} \lambda_i \delta_{x_i}$ on $M$,
that is to say,
% then from previous discussion we have
\[
	B(\boldsymbol{x}) \in \arg \min_{w \in M} \sum_{i=1}^{n} \lambda_i d(w, x_i)^2.
\]
% Let $T_i$ be the unique optimal transfer map from $\bar{\mu}$ to $\mu_i$,
% and define $\gamma := (T_1, T_2, \ldots, T_n)_{\#} \bar{\mu}$ to be the push-forward measure of $\bar{\mu}$ on $M^n$.

% Then for $(x_1, x_2, \ldots, x_n)$ in the support of $\gamma$,
% there is $\gamma$-a.e. a unique barycenter $B(x_1, x_2, \ldots, x_n)$ of measure $\sum_{j=1}^n \lambda_j \delta_{x_j}$.
% Both map $B$ and $f$ are $\gamma$-a.e. differentiable.
% The map $B(T_1, T_2, \ldots, T_n)$ is $\bar{\mu}$-a.e. the identity map on $M$.

We have following \textbf{barycenter formula} holds almost everywhere for volume measure,
more exactly, where hessian of $f_i$ exists,
% if $\mu_i$ is absolutey continous
\begin{equation}
	\label{formula_barycenter}
	B = \exp (- \frac{1}{\lambda_i} \nabla_i f).
\end{equation}
% where $\nabla_i f$ means partial gradient of $f$,
% i.e., $\nabla_i f = \nabla f_i$ if

% \begin{proof}
% We intend to apply discussion of conditional probability in \cref{discussion_conditional_prob} for the case
% that $\gamma$ is absolutely continous.
% Firstly we should prove that $\gamma$ is absolutely continous,
% and we may get inspiration from \cite{santambrogio2009absolute} for the case of $n=2$.
% If this is done, we can apply previous discussion.

% \end{proof}

\begin{rmk}
	Some comments about this formula \cref{formula_barycenter}:
	\begin{itemize}
		\item The barycenter formula \cref{formula_barycenter} expresses the fact that,
		      from point $x_i$,
		      we can reach barycenter of $\sum_{j=1}^n \lambda_i \delta_{x_j}$ if we follow
		      the direction $-\nabla_i f$
		      and advance $\| \nabla_i f / \lambda_i \| $.
		\item In the case of Euclidean space, $\nabla_i (f/\lambda_i)(x_i)$ reduces to
		      $x_i - \sum_{j=1}^n \lambda_j x_j$.
		      % \item $\exp(-\nabla f_i / \lambda_i )$ is the optimal transfer map from $\mu_i$ to $\bar{\mu}$.
		% \item Set $g(z) := -\sum_{j\neq i}^n \lambda_j c(z, x_j) / \lambda_i$,
		% 	then $f_i / \lambda_i = g^c$ and $ (f_i / \lambda_i )^c \geq g$.
		\item In the case of Eculidean space, $\nabla (f_i / \lambda_i)^c (z) = \nabla g_i(z) $ reduces to

		      $(z - \sum_{j=1}^n \lambda_j x_j) / \lambda_i -(z-x_i)$.
	\end{itemize}
\end{rmk}

% Aim: try to estimate location of barycenter

% \textcolor{red}{We have to fix $i$ and assume discrete measure for others,
% 	since first order balance holds in the case of Euclidean space only
% 	if $n=1$.}
% It is possible to get a inverse to $B$.
% without absolutely continuity of $\bar{\mu}$.
% To do so, we need to fix $n-1$ variables of $B$.
% We define a barycenter map from $M$ to $M$, 
% $br(x_1) := B(x_1, x^\prime) = \exp(-\frac{1}{\lambda_1}\nabla_1 f),$
% where $x^\prime := (x_2, x_3, \ldots, x_n)$;
% and we define
% $
% g ( z ) = -(f_1 / \lambda_1)^c=
% 	-\frac { 1 } { \lambda _ { 1 } } \sum _ { i = 2 } ^ { n } \lambda _ { i } \, c\left( x _ { i } , z \right),
% $
% Recall that for each \( x \),
% every \( z \in br\left( x , x _ { 2 } , \ldots . . x _ { n } \right) \) is not in the cut-locus of any \( x _ { i } \).
% Therefore, \( g ( z ) \) is twice differentiable at each \( z \in br\left( M , x _ { 2 } , \ldots , x _ { n } \right) . \)

% Now, for any point \( y \) such that
% \[ z \in br\left( y , x _ { 2 } , \ldots , x _ { n } \right) \]
% from the definition of \( g ( z ) \), we have \( \left. \nabla _ { w } \right| _ { w = z } c( y , w ) = - \nabla _ { z } g ( z ) \) or, equivalently,
% \( y = \exp _ { z } \nabla g ( z ) . \)
% Therefore, \( \exp _ { z } \nabla g ( z ) \) is the only point with the desired property.
% The good news of $g_i$ is that
% we don't need to argue its properties through abstract $c$-concave function.
The function $g_i = f_i / \lambda_i $ is by definition just a sum of squared distance functions.
Now we aim to find out when \( \exp(- \nabla g_i ) = \exp(- \nabla (f_i / \lambda_i)^c) \) is a Lipschitz function.
% with Lipschitz constant depending only on $M$ and $\lambda_i$.
% As $M$ is compact, though tangent bundle $TM$ is not compact, local Lipschitz plus bounded diameter of $M$ implies global Lipschitz of $\exp$.
% Function $g_i$ has hessian upper bound by \cref{prop:differentiate_optimal_transport},
% here we treat it as an infimal convolution over a fixed sigleton set $\{x_i\}$.
% Note that from $ \nabla g_i = \nabla (f_i / \lambda_i)^c$,
% $g_i$ and $(f_i / \lambda_i)^c$ have the same hessian.
Function $(f_i / \lambda_i)^c$ has hessian upper bound as a $c$-conjugate function;
function $g_i$ has hessian lower bound as a negative linear combination of square distance function.
We then have
\begin{equation}
	\label{equa:hessian_bound_f}
	-\frac{1-\lambda_i}{\lambda_i} H \leq \nabla^2 (f_i / \lambda_i)^c \leq H,
\end{equation}
where we denote by $H$ a possible bound from above for the hessian of square distance funtion.
% Recall that $H$ is bounded from above.
% Squared distance function $c$ has bounded Hessian from above.
% Hence $ \| \nabla^2 g \|$ is bounded from above by taking the second derivative of definition and also applying minimallity of cost at barycenter.

Hence, we need two properties to have Lipschitzness of $\exp(-\nabla (f_i / \lambda_i)^c)$:
\begin{itemize}
	\item $ \exp $ is Lipschitz on the domain we are interested in.
	      % This would be the case when we are in the support of a compact supported function.
	      For example, in a compact domain.
	\item The hessian of square distance function has upper bound $H$.
	      This is the case when we have lower bound of sectional curvature.
\end{itemize}

We always take the second condition in following discussion.
As we actually only consider measures with compact support, the first assumption is fullfilled as well.

